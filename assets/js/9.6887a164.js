(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{407:function(v,_,e){"use strict";e.r(_);var t=e(56),a=Object(t.a)({},(function(){var v=this,_=v.$createElement,e=v._self._c||_;return e("ContentSlotsDistributor",{attrs:{"slot-key":v.$parent.slotKey}},[e("h1",{attrs:{id:"人工智能导论"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#人工智能导论"}},[v._v("#")]),v._v(" 人工智能导论")]),v._v(" "),e("p",[v._v("Intro to AI")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("think like humam")])]),v._v(" "),e("li",[e("p",[v._v("think rationally")])]),v._v(" "),e("li",[e("p",[v._v("act like human")])]),v._v(" "),e("li",[e("p",[v._v("act rationally")])])]),v._v(" "),e("h2",{attrs:{id:"一、搜索与规划"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#一、搜索与规划"}},[v._v("#")]),v._v(" 一、搜索与规划")]),v._v(" "),e("blockquote",[e("p",[v._v("Search and Planning")])]),v._v(" "),e("h3",{attrs:{id:"_1、search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、search"}},[v._v("#")]),v._v(" 1、Search")]),v._v(" "),e("h4",{attrs:{id:"_1-1、uninformed-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1、uninformed-search"}},[v._v("#")]),v._v(" 1.1、Uninformed Search")]),v._v(" "),e("p",[v._v("Search Properties")]),v._v(" "),e("ul",[e("li",[v._v("State")]),v._v(" "),e("li",[v._v("Successor")]),v._v(" "),e("li",[v._v("Start")]),v._v(" "),e("li",[v._v("Solution（Goal）")])]),v._v(" "),e("p",[v._v("State Space Graphs and Search Trees")]),v._v(" "),e("ul",[e("li",[v._v("node")]),v._v(" "),e("li",[v._v("expand")]),v._v(" "),e("li",[v._v("fringe")])]),v._v(" "),e("p",[v._v("Search Strategy")]),v._v(" "),e("ul",[e("li",[v._v("DFS（深度优先搜索）")]),v._v(" "),e("li",[v._v("BFS（广度优先搜索）")]),v._v(" "),e("li",[v._v("IDS（迭代加深搜索）")]),v._v(" "),e("li",[v._v("UCS（统一代价搜索）")])]),v._v(" "),e("p",[v._v("Search Algorithm Properties（属性）")]),v._v(" "),e("ul",[e("li",[v._v("完备性")]),v._v(" "),e("li",[v._v("最优性")]),v._v(" "),e("li",[v._v("时间复杂度")]),v._v(" "),e("li",[v._v("空间复杂度")])]),v._v(" "),e("p",[v._v("When the search tree is finite")]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Strategy")]),v._v(" "),e("th",[v._v("space")]),v._v(" "),e("th",[v._v("complete")]),v._v(" "),e("th",[v._v("optimal")]),v._v(" "),e("th",[v._v("time")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("DFS")]),v._v(" "),e("td",[v._v("O(bm)")]),v._v(" "),e("td",[v._v("√")]),v._v(" "),e("td",[v._v("×")]),v._v(" "),e("td",[v._v("O(b^m)")])]),v._v(" "),e("tr",[e("td",[v._v("BFS")]),v._v(" "),e("td",[v._v("O(b^s)")]),v._v(" "),e("td",[v._v("√")]),v._v(" "),e("td",[v._v("×")]),v._v(" "),e("td",[v._v("O(b^s")])])])]),v._v(" "),e("p",[v._v("Trade-Offs BFS&DFS （权衡）")]),v._v(" "),e("p",[v._v("1、Iterative Deepening Search：迭代加深搜索")]),v._v(" "),e("ul",[e("li",[v._v("base on layer")])]),v._v(" "),e("blockquote",[e("p",[v._v("一层一层向下搜的意思")]),v._v(" "),e("p",[v._v("这样会造成浪费，比如第一次搜5层，第二次搜10层，第二次搜索仍旧需要经历第一次搜索的前五层")]),v._v(" "),e("p",[v._v("但这种浪费并不是特别大，每加的一层是所有前面层数之和，何不忽略一下")])]),v._v(" "),e("ul",[e("li",[v._v("always run dfs")]),v._v(" "),e("li",[v._v("cap the depth you are willing to search when get the depth")]),v._v(" "),e("li",[v._v("if get the solution then return, if not, going on searching the next depth")])]),v._v(" "),e("p",[v._v("2、Uniform Cost Search：统一代价搜索")]),v._v(" "),e("ul",[e("li",[v._v("base on cost so far")])]),v._v(" "),e("blockquote",[e("p",[v._v("永远选择代价最小的节点优先扩展")]),v._v(" "),e("p",[v._v("当找到Goal时，记录他的代价cost，继续搜索代价还没到达cost的路径，直到所有路径的代价都大于等于cost，返回当前Goal")])]),v._v(" "),e("ul",[e("li",[v._v("expand a cheapest node first")]),v._v(" "),e("li",[v._v("fringe is priority queue")])]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Strategy")]),v._v(" "),e("th",[v._v("space")]),v._v(" "),e("th",[v._v("time")]),v._v(" "),e("th",[v._v("optimal")]),v._v(" "),e("th",[v._v("complete")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("UCS")]),v._v(" "),e("td",[v._v("O(b^(*/ε))")]),v._v(" "),e("td",[v._v("O(b^(*/ε))")]),v._v(" "),e("td",[v._v("√")]),v._v(" "),e("td",[v._v("√")])])])]),v._v(" "),e("ul",[e("li",[v._v("Issues: it explores every direction, which could be very expensive")]),v._v(" "),e("li",[v._v("Solution: focus on things that are promising（有希望的）rather than that have been cheap so far")])]),v._v(" "),e("p",[v._v("What we should do for example is that use one piece of code to maintains a priority queue by different strategy when facing different situation, Such as using DFS when the search tree is deep, using BFS when the tree is shallow, using UCS when particularly considering the cost so far on current path")]),v._v(" "),e("h4",{attrs:{id:"_1-2、infomed-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2、infomed-search"}},[v._v("#")]),v._v(" 1.2、Infomed Search")]),v._v(" "),e("blockquote",[e("p",[v._v("截止日期放到周五下午四点能有效帮助度过一个舒服的周末")]),v._v(" "),e("p",[v._v("搜索算法是系统的构建搜索树的方法，希望在搜索树的一部分便找到答案，最差的情况是构建了整个搜索树")])]),v._v(" "),e("p",[v._v("不同的搜索算法的实际区别其实就是扩展边缘的方法不同")]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Algorithm")]),v._v(" "),e("th",[v._v("check")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("DFS")]),v._v(" "),e("td",[v._v("stack")])]),v._v(" "),e("tr",[e("td",[v._v("BFS")]),v._v(" "),e("td",[v._v("queue")])]),v._v(" "),e("tr",[e("td",[v._v("some other")]),v._v(" "),e("td",[v._v("priority queue")])])])]),v._v(" "),e("h5",{attrs:{id:"_1-2-1、heurisics-启发式"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-1、heurisics-启发式"}},[v._v("#")]),v._v(" 1.2.1、heurisics（启发式）")]),v._v(" "),e("blockquote",[e("p",[v._v("estimate of distance to nearest goal for each state")])]),v._v(" "),e("p",[v._v("heurisics function（启发函数）：测量距离goal的距离")]),v._v(" "),e("p",[v._v("通常使用减法而不是测量已走过的距离，即计算剩余的距离，直到距离为"),e("code",[v._v("0")]),v._v("找到"),e("code",[v._v("goal")])]),v._v(" "),e("p",[v._v("启发式只是一种估算，当遇到非常糟糕的启发式，估算结果可能会非常离谱")]),v._v(" "),e("h5",{attrs:{id:"_1-2-2、greedy-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-2、greedy-search"}},[v._v("#")]),v._v(" 1.2.2、Greedy Search")]),v._v(" "),e("p",[v._v("通过启发式函数找到最接近goal的fringe，进行扩展")]),v._v(" "),e("p",[v._v("不同于一致代价搜索，一致代价搜索是找的代价最小的fringe，即深度最小的进行搜索，是盲目的，而贪婪搜索基于当前node到goal的距离进行有信息的搜索")]),v._v(" "),e("p",[v._v("选择局部最有利的节点进行扩展，忽略已耗费的代价，很有可能不是全局最有利的，甚至有可能走到头距离不能为0，即找不到goal")]),v._v(" "),e("p",[v._v("ucs is the tortoise（龟）, and greedy is the hare（兔）")]),v._v(" "),e("h5",{attrs:{id:"_1-2-3、a-star-search-a-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-3、a-star-search-a-search"}},[v._v("#")]),v._v(" 1.2.3、A Star Search (A* Search)")]),v._v(" "),e("p",[v._v("combine the tortoise and hare")]),v._v(" "),e("p",[v._v("同时考虑代价（g）和距目标距离（h），选择"),e("code",[v._v("min(Node.g+Node.h)")]),v._v("的节点进行扩展，当找到goal时（s=0），pop出节点得到结果"),e("code",[v._v("d")]),v._v("（实际距离），不宣布成功，继续选择"),e("code",[v._v("n")]),v._v("节点满足"),e("code",[v._v("f(n)<d")]),v._v("进行扩展，当新弹出的节点实际路径小于"),e("code",[v._v("d")]),v._v("，更新"),e("code",[v._v("d")]),v._v("，直到没有节点满足"),e("code",[v._v("f(n)<d")]),v._v("时，宣布成功")]),v._v(" "),e("p",[v._v("当实际成本小于估计成本，即预估"),e("code",[v._v("h")]),v._v("偏大，A*搜索很可能会略过这样的节点从而丢失最优解")]),v._v(" "),e("p",[v._v("可容的（乐观的）启发式：其预估成本总小于或等于实际成本")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("如用城市的直线距离作为启发方式是一种乐观启发")])]),v._v(" "),e("li",[e("p",[v._v("0 <= h(n) <= h*(n)")]),v._v(" "),e("p",[v._v("每个节点的启发式值低于或等于从当前节点到目标成本最少的路径大小")]),v._v(" "),e("p",[v._v("当h(n)为0时即为统一代价搜索")]),v._v(" "),e("p",[v._v("当h(n)越接近h*(n)，效率越高")])])]),v._v(" "),e("p",[v._v("当使用的启发式是可接受的，那么我们说A*的树搜索将是最佳的")]),v._v(" "),e("p",[v._v("由于预计成本总小于实际成本，或许"),e("code",[v._v("b")]),v._v("已经很接近"),e("code",[v._v("goal")]),v._v("，但它已消耗的代价"),e("code",[v._v("h(b)")]),v._v("加上启发式"),e("code",[v._v("g(b)")]),v._v("很可能大于一个离"),e("code",[v._v("goal")]),v._v("很远的节点"),e("code",[v._v("a")]),v._v("，因为"),e("code",[v._v("h(a)")]),v._v("必然小于"),e("code",[v._v("h(b)")]),v._v("，估计值永远不高于实际值，很有可能"),e("code",[v._v("h(a)+g(a)f(a)<f(b)=h(b)+g(b)")])]),v._v(" "),e("p",[e("strong",[v._v("Proof:")])]),v._v(" "),e("ul",[e("li",[v._v("b is on the fringe")]),v._v(" "),e("li",[v._v("n is one of ancestor of a and on the fringe")]),v._v(" "),e("li",[v._v("Claim: n will be expanded before b\n"),e("ol",[e("li",[v._v("f(n) is less than f(b)")]),v._v(" "),e("li",[v._v("f(a) is less than f(b)")]),v._v(" "),e("li",[v._v("n expanded before b")])])]),v._v(" "),e("li",[v._v("all ancestors of a will expand before b")]),v._v(" "),e("li",[v._v("a will expand before b")]),v._v(" "),e("li",[v._v("A* search is optimal")])]),v._v(" "),e("p",[v._v("当"),e("code",[v._v("b")]),v._v("和"),e("code",[v._v("n")]),v._v("同时存在于"),e("code",[v._v("fringe")]),v._v("，"),e("code",[v._v("n")]),v._v("是"),e("code",[v._v("a")]),v._v("祖先，若"),e("code",[v._v("g(n)<g(b)、f(a)<=f(n)")]),v._v("，又必然"),e("code",[v._v("h(a)=h(b)")]),v._v("，在此时一定会选择"),e("code",[v._v("n")]),v._v("扩展，达到节点"),e("code",[v._v("a")]),v._v("，同理若"),e("code",[v._v("f(c)<f(n)")]),v._v("，一定也会往"),e("code",[v._v("c")]),v._v("扩展")]),v._v(" "),e("p",[v._v("不可能发生先到达"),e("code",[v._v("b")]),v._v("再到达"),e("code",[v._v("a")]),v._v("的情况，因为他们总拥有公共祖先，于是具有可容启发式的A*搜索获得了最优性")]),v._v(" "),e("p",[e("strong",[v._v("8 Puzzle:")])]),v._v(" "),e("p",[v._v("启发式：在一个A*搜索中，你可以使用多个可容的启发式，对于同一个节点，选择其最大的启发式值作为"),e("code",[v._v("h(n)")])]),v._v(" "),e("ul",[e("li",[v._v("位于错误位置的拼图数量（一定小于他所需要移动的次数）")]),v._v(" "),e("li",[v._v("总曼哈顿距离，即没有障碍，各个拼图移动到正确位置步数总和")])]),v._v(" "),e("p",[v._v("后继：空位相邻的滑块滑向空位")]),v._v(" "),e("h5",{attrs:{id:"_1-2-4、graph-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-4、graph-search"}},[v._v("#")]),v._v(" 1.2.4、Graph Search")]),v._v(" "),e("blockquote",[e("p",[v._v("永远不要扩展状态两次")])]),v._v(" "),e("p",[v._v("在A*搜索的基础上，维护一个"),e("code",[v._v("closed")]),v._v("集合记录已经扩展过的节点，在每次扩展时，检查一遍"),e("code",[v._v("closed")]),v._v("集合，如果包含，"),e("code",[v._v("skip it")])]),v._v(" "),e("p",[v._v("这里数据结构选择"),e("code",[v._v("set")]),v._v("而不是"),e("code",[v._v("list")]),v._v("，会导致很慢捏")]),v._v(" "),e("p",[v._v("这样会有一个什么问题：第一次选择了一条没走过但很远的路，导致你一定忽略一些很短但已经被走过的路（如果有）。第一次选择必须十分谨慎")]),v._v(" "),e("p",[v._v("为了避免上述问题，我们使用一致性的启发式搜索，又称单调性")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("h(n) <= c(n, a, n1) + h(n1)")]),v._v(" "),e("p",[v._v("h(n)表示节点n的启发函数值，c(n, a, n1)表示从n经过动作a到达n1的代价，h(n1)为节点n1的启发值")])]),v._v(" "),e("li",[e("p",[v._v("简单来说，就是说启发函数随着节点推移，一定是递增的")])]),v._v(" "),e("li",[e("p",[v._v("这也意味着，你的每一步都将使代价提升，无法后退")])])]),v._v(" "),e("p",[v._v("于是具有一致性启发式的图搜索获得了最优性")]),v._v(" "),e("p",[v._v("由上可见，高效的启发式搜索是决定性能的关键")]),v._v(" "),e("h3",{attrs:{id:"_2、csps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、csps"}},[v._v("#")]),v._v(" 2、CSPs")]),v._v(" "),e("blockquote",[e("p",[v._v("搜索得前提是世界是确定的")]),v._v(" "),e("p",[v._v("Constraint Satisfaction Problems")]),v._v(" "),e("p",[v._v("约束满足问题")])]),v._v(" "),e("h4",{attrs:{id:"_2-1、what-is-csps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1、what-is-csps"}},[v._v("#")]),v._v(" 2.1、What is CSPs")]),v._v(" "),e("p",[v._v("搜索")]),v._v(" "),e("ul",[e("li",[v._v("规划问题")]),v._v(" "),e("li",[v._v("识别问题")])]),v._v(" "),e("p",[v._v("不同于标准搜索问题，标准搜索是黑盒的，只有起始和后继，中间发生了什么是位置的")]),v._v(" "),e("p",[v._v("而CSP问题常有以下元素")]),v._v(" "),e("ul",[e("li",[v._v("Variables")]),v._v(" "),e("li",[v._v("Domains")]),v._v(" "),e("li",[v._v("Constraints\n"),e("ul",[e("li",[v._v("Implicit")]),v._v(" "),e("li",[v._v("Explicit")]),v._v(" "),e("li",[v._v("Unary/Binary/N-ary")])])])]),v._v(" "),e("p",[v._v("Goals")]),v._v(" "),e("ul",[e("li",[v._v("Here：find any solution")]),v._v(" "),e("li",[v._v("Also：find all, find best. etc")])]),v._v(" "),e("p",[v._v("约束限制着域名的选择")]),v._v(" "),e("p",[v._v("Map Coloring")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("变量：WA, NT, 1, NSW, V, SA, T")])]),v._v(" "),e("li",[e("p",[v._v("域名：red, green, blue")])]),v._v(" "),e("li",[e("p",[v._v("约束")]),v._v(" "),e("ul",[e("li",[v._v("implicit: WA != NT")]),v._v(" "),e("li",[v._v("explicit: (WA, NT) ∈ {(red, green), (red, blue), (green, red) ...}")]),v._v(" "),e("li",[v._v("即相邻的不同变量颜色不能相同")])])]),v._v(" "),e("li",[e("p",[v._v("解决方案：")]),v._v(" "),e("p",[v._v("{WA=red, NT=green, Q=red, NSW=green, V=red, SA=blue, T=green}")])])]),v._v(" "),e("p",[v._v("N-Queens")]),v._v(" "),e("ul",[e("li",[v._v("变量：Xij")]),v._v(" "),e("li",[v._v("域名：{0, 1}")]),v._v(" "),e("li",[v._v("约束：和平的王国")])]),v._v(" "),e("p",[v._v("Constraint Graphs：约束图")]),v._v(" "),e("ul",[e("li",[v._v("arch show constraint")]),v._v(" "),e("li",[v._v("variables joined by arch")]),v._v(" "),e("li",[v._v("domain selected by state")])]),v._v(" "),e("p",[v._v("矩形表示约束，弧线表示联系（即存在约束），连接了状态")]),v._v(" "),e("p",[v._v("The Waltz Algorithm")]),v._v(" "),e("blockquote",[e("p",[v._v("根据平面画出3D图")])]),v._v(" "),e("ul",[e("li",[v._v("CSPs are everywhere")])]),v._v(" "),e("p",[v._v("试想如何解决CSPs，以地图上色为例")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("若用广度优先搜索")]),v._v(" "),e("p",[v._v("很明显，所有的解都在最后一层，因为每一层只能给一个州上色，直到底部才能上色完毕，计算量极大")])]),v._v(" "),e("li",[e("p",[v._v("若用深度优先搜索")]),v._v(" "),e("p",[v._v("若是盲目深度搜索，他只会不停给每一层某个state上色后继续加深而不管任何约束，能很快找到不满足约束的解，但这很明显不能解决问题")])])]),v._v(" "),e("h4",{attrs:{id:"_2-2、backtracking-serach"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2、backtracking-serach"}},[v._v("#")]),v._v(" 2.2、Backtracking Serach")]),v._v(" "),e("p",[v._v("回溯搜索")]),v._v(" "),e("blockquote",[e("p",[v._v("based on dfs")])]),v._v(" "),e("p",[v._v("每次扩展时问一遍自己违反了规则吗，若违反则选择不扩展（说明这个节点之下的整棵树都是错误的），转而考虑其他节点进行扩展")]),v._v(" "),e("p",[v._v("通常使用递归回溯代码进行实现")]),v._v(" "),e("ol",[e("li",[v._v("以某种顺序选择一个未被遗弃的节点")]),v._v(" "),e("li",[v._v("检查\n"),e("ul",[e("li",[v._v("违反约束：回溯，遗弃这颗子树")]),v._v(" "),e("li",[v._v("不违反：loop")]),v._v(" "),e("li",[v._v("找到答案：return")])])])]),v._v(" "),e("h5",{attrs:{id:"_2-2-1、filtering"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-1、filtering"}},[v._v("#")]),v._v(" 2.2.1、Filtering")]),v._v(" "),e("p",[v._v("前瞻搜索")]),v._v(" "),e("p",[v._v("再回溯搜索的基础上，过滤你的候选项，跟踪未分配的域名，进行"),e("code",[v._v("forward checking")]),v._v("，检查当前变量们是否都满足约束")]),v._v(" "),e("ul",[e("li",[v._v("domain cloud")]),v._v(" "),e("li",[v._v("forward checking")])]),v._v(" "),e("p",[v._v("但注意他只会检查立即发生的错误，即只推演一步，而不是多步")]),v._v(" "),e("p",[v._v("同时约束只会传递到相邻的state")]),v._v(" "),e("p",[v._v("这就意味着，只有当问题立即发生，即某个state没有domain选择了，前瞻搜索才会发现出错，这时将进行一个大回溯")]),v._v(" "),e("p",[v._v("直到回溯到，弄清楚到底是哪一步就注定了这次失败（有可能是一开始或是第二步），再重新进行选择")]),v._v(" "),e("ul",[e("li",[v._v("it's already doomed")])]),v._v(" "),e("p",[e("strong",[v._v("Constraint Propagation")])]),v._v(" "),e("blockquote",[e("p",[v._v("约束传递，约束传递越广，前瞻性越好，能更早的发现问题避免回溯")]),v._v(" "),e("ul",[e("li",[v._v("Constraint Propagation")]),v._v(" "),e("li",[v._v("Forward Checking")])])]),v._v(" "),e("p",[v._v("约束一致性")]),v._v(" "),e("ul",[e("li",[v._v("arc consistency")])]),v._v(" "),e("p",[v._v("提前消除冲突的约束，每次做出决定后，对所有后继节点进行检查，消除其有冲突的后继")]),v._v(" "),e("ul",[e("li",[v._v("arc police")])]),v._v(" "),e("p",[v._v("每次删除某个domain时，都要对尚未决定的所有节点重新检查，满足约束的一致性")]),v._v(" "),e("ul",[e("li",[v._v("即强行执行约束一致性")])]),v._v(" "),e("p",[v._v("AC3 Algorithm")]),v._v(" "),e("ul",[e("li",[v._v("Two loops")]),v._v(" "),e("li",[v._v("当每次做出删除操作，内循环需要重置")])]),v._v(" "),e("p",[v._v("但他本质上还是一个回溯算法，当强制执行约束一致性无法继续时，仍需要回溯找其他办法")]),v._v(" "),e("h5",{attrs:{id:"_2-2-2、ordering"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-2、ordering"}},[v._v("#")]),v._v(" 2.2.2、Ordering")]),v._v(" "),e("blockquote",[e("p",[v._v("排序的目的：加速算法")])]),v._v(" "),e("p",[e("strong",[v._v("Minimum Remaining Values")])]),v._v(" "),e("p",[v._v("在前瞻搜索的基础上，总是选择值域最少的节点进行扩展")]),v._v(" "),e("p",[v._v("先找到问题的棘手部分")]),v._v(" "),e("ul",[e("li",[v._v("most constrained variable最多受限变量")]),v._v(" "),e("li",[v._v("fail-fast机制，有效减少回溯深度")])]),v._v(" "),e("p",[e("strong",[v._v("Least Constraining Value")])]),v._v(" "),e("p",[v._v("最小约束价值")]),v._v(" "),e("h5",{attrs:{id:"_2-2-3、consistency"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-3、consistency"}},[v._v("#")]),v._v(" 2.2.3、Consistency")]),v._v(" "),e("blockquote",[e("p",[v._v("graph structure")])]),v._v(" "),e("p",[v._v("Arc Consistency and Beyond：检查弧和图形的一致性，即对于节点约束的双方，约束都是能够满足的")]),v._v(" "),e("p",[v._v("比如V有蓝、绿色选，NSW有蓝色选，二者存在约束，此时他们满足弧的一致性，因为NSW选择了蓝色，V仍有绿色可以选择；但若SA只有蓝色选，NT只有蓝色选，二者存在约束，那么他们不满足弧的一致性，这时则需要回溯")]),v._v(" "),e("p",[v._v("与前瞻搜索的区别在于：forward checking只有在问题发生时才能检测到，而arc consistency将领先一步")]),v._v(" "),e("p",[v._v("Limitations of Arc Consistency")]),v._v(" "),e("p",[v._v("After enforcing arc consistency")]),v._v(" "),e("ul",[e("li",[v._v("Can have on solution left")]),v._v(" "),e("li",[v._v("Can have multiple solutions left")]),v._v(" "),e("li",[v._v("Can have no solutions left and not know it（go backtracking）")])]),v._v(" "),e("p",[v._v("Arc consistency still runs inside a backtracking search")]),v._v(" "),e("p",[v._v("注意每次进行约束判断后，都需要对以往变量重新进行检查，这很昂贵但很有用")]),v._v(" "),e("p",[v._v("K-Consistency")]),v._v(" "),e("ul",[e("li",[v._v("K=1，代价是无，即保持自身的约束一致性，只要有任意合法域名，便保持1-consistency，代价可以说是无")]),v._v(" "),e("li",[v._v("K=2，即为保持弧度一致性")]),v._v(" "),e("li",[v._v("K=3，Path Consistency")])]),v._v(" "),e("p",[v._v("Strong K-Consistency")]),v._v(" "),e("h5",{attrs:{id:"_2-2-4、structure"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-4、structure"}},[v._v("#")]),v._v(" 2.2.4、Structure")]),v._v(" "),e("blockquote",[e("p",[v._v("图表的结构可以照亮方式")])]),v._v(" "),e("p",[v._v("problem structure")]),v._v(" "),e("h6",{attrs:{id:"_1independent-sub-problem"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1independent-sub-problem"}},[v._v("#")]),v._v(" ①Independent Sub-Problem")]),v._v(" "),e("p",[v._v("存在独立的子问题，如地图着色中的岛屿，拉出来单独考虑")]),v._v(" "),e("h6",{attrs:{id:"_2tree-structured-csps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2tree-structured-csps"}},[v._v("#")]),v._v(" ②Tree-Structured CSPs")]),v._v(" "),e("p",[v._v("no loops：没有循环")]),v._v(" "),e("p",[v._v("only on parent：只有一个父母节点，这决定了只需满足上下二者的弧一致性，并一直扩展到叶子，那么一定是满足约束的")]),v._v(" "),e("p",[v._v("时间复杂度随变量增多线性增加")]),v._v(" "),e("ol",[e("li",[e("p",[v._v("order：选择一个节点作为根节点")])]),v._v(" "),e("li",[e("p",[v._v("Remove backward：对其余所有节点进行弧一致性约束")]),v._v(" "),e("p",[v._v("for i = n: 2, apple RemoveInconsistent(Parent(xi). xi)")])]),v._v(" "),e("li",[e("p",[v._v("Assign forward：选择未被选择的节点，开始扩展")])])]),v._v(" "),e("p",[v._v("Runtime：O(n d^2)")]),v._v(" "),e("p",[v._v("这样做之后，从根节点到叶子节点的每一条路径都是弧一致的，因为我们访问了每个子节点并Make it arc-consistency")]),v._v(" "),e("h6",{attrs:{id:"_3nearly-tree-structured-csps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3nearly-tree-structured-csps"}},[v._v("#")]),v._v(" ③Nearly Tree-Structured CSPs")]),v._v(" "),e("p",[e("strong",[v._v("delete")])]),v._v(" "),e("p",[v._v("Cutest conditioning Algorithm")]),v._v(" "),e("ul",[e("li",[v._v("Conditioning：实例化一个变量时，考虑对其邻居的影响，那么当前节点和他的邻居便形成了一种树状结构（残差图），对其邻居的域名进行修剪（prune）")]),v._v(" "),e("li",[v._v("Cutest condition：实例化一组变量，直到残差图是一棵树")]),v._v(" "),e("li",[v._v("Cutest size c gives runtime O((d^c)(n-c)d^2)，very fast for small c（c应该是实例化的那组变量）")])]),v._v(" "),e("p",[v._v("那么现在的问题就变成了如何高效地进行剪裁，尽量少地实例化变量而得到一颗树结构的残差图，转化为一个树结构的CSP问题，再使用树结构求解算法")]),v._v(" "),e("p",[v._v("delete node until have a tree left")]),v._v(" "),e("p",[e("strong",[v._v("grouping")])]),v._v(" "),e("p",[v._v("以当前节点为中心，对邻居进行分组")]),v._v(" "),e("p",[v._v("如当前选择了SA，其余均与其有约束，进行如下分组："),e("code",[v._v("{SA, WQA, NT}, {SA, NT, Q}, {SA, Q, NSW}, {SA, NSW, V}")])]),v._v(" "),e("p",[v._v("不能完全单独解决这些子问题，要有一些限制：如相同的变量必须保持相同的值")]),v._v(" "),e("ul",[e("li",[v._v("选择必须是连贯的")])]),v._v(" "),e("h4",{attrs:{id:"_2-3、local-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3、local-search"}},[v._v("#")]),v._v(" 2.3、Local Search")]),v._v(" "),e("blockquote",[e("p",[v._v("局部搜索")]),v._v(" "),e("p",[v._v("记住解决CSP问题的基础仍是搜索，这是一个不断优化的过程")])]),v._v(" "),e("p",[v._v("Local Serach的思路是：先完整再合法。即先搜索到某一个叶子节点，该路径并不合法，再对其进行矫正使其合法或失败，这完全不同于之前的搜索，即先保证合法，才能继续搜索")]),v._v(" "),e("p",[v._v("即先凑出一个答案，再慢慢修改答案使之合法")]),v._v(" "),e("h5",{attrs:{id:"_2-3-1、iterative-improvement"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-1、iterative-improvement"}},[v._v("#")]),v._v(" 2.3.1、Iterative Improvement")]),v._v(" "),e("blockquote",[e("p",[v._v("迭代改进")])]),v._v(" "),e("p",[v._v("apply to CSPs")]),v._v(" "),e("ul",[e("li",[v._v("Take an assignment with unsatisfied constraints")]),v._v(" "),e("li",[v._v("Operators reassign variable values")]),v._v(" "),e("li",[v._v("No fringe! Live on the edge")])]),v._v(" "),e("p",[e("code",[v._v("Operator")]),v._v("对应"),e("code",[v._v("successor function")]),v._v("，其功能为：重新分配变量值")]),v._v(" "),e("p",[v._v("Algorithm")]),v._v(" "),e("ul",[e("li",[v._v("Variable selection: randomly select and conflicted variable")]),v._v(" "),e("li",[v._v("Value selection: "),e("strong",[v._v("min-conficts heuristic")]),v._v(" "),e("ul",[e("li",[v._v("选择违反的价值最少的限制，即最大的减少冲突")]),v._v(" "),e("li",[v._v("直到减少的冲突等于违反约束的总数")])])])]),v._v(" "),e("p",[v._v("找寻变量规则：最小冲突启发式")]),v._v(" "),e("p",[v._v("举个例子：有人和你起了冲突要宰你，你有一个机会跑路，那么你总是跑到离他最远的地方。这个地方便是你选择的变量值")]),v._v(" "),e("ul",[e("li",[v._v("这有一个致命问题：有可能死锁，即有两个或多个冲突相联系，即解决了1出现了2，解决了2又出现1")])]),v._v(" "),e("p",[v._v("适用情况：")]),v._v(" "),e("ul",[e("li",[v._v("变量很多，约束很少")]),v._v(" "),e("li",[v._v("约束很多")])]),v._v(" "),e("p",[v._v("why?")]),v._v(" "),e("ul",[e("li",[v._v("当约束很少，你可以随便选，而且很少触发冲突")]),v._v(" "),e("li",[v._v("当约束很多，你的选择极少，可能只有一两种改进方法，这也会算的很快")])]),v._v(" "),e("h5",{attrs:{id:"_2-3-2、hill-climbing"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-2、hill-climbing"}},[v._v("#")]),v._v(" 2.3.2、Hill Climbing")]),v._v(" "),e("p",[v._v("从一个初始点出发，试着让他变得更好，当无法变得更好时（冲突不会继续减少）， stop and declared finished")]),v._v(" "),e("ul",[e("li",[v._v("这无法保证搜索完整性和最优性")])]),v._v(" "),e("p",[v._v("就如爬山时始终朝着比自己高的地方爬，很有可能你会停在一个小山包而不是山顶，是一个局部最大值")]),v._v(" "),e("p",[v._v("可怕之处是局部最大值和全局最大值对于身处环境根本没有特征能够区别开来（no sign）")]),v._v(" "),e("p",[v._v("所以本地搜索有很多并行运行的线程，从不同初始点开始搜索，希望找到不同的局部最大值，从而找到全局最大值")]),v._v(" "),e("h5",{attrs:{id:"_2-3-3、simulated-annealing"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-3、simulated-annealing"}},[v._v("#")]),v._v(" 2.3.3、Simulated Annealing")]),v._v(" "),e("blockquote",[e("p",[v._v("退火模拟")])]),v._v(" "),e("p",[v._v("核心思想：jump around")]),v._v(" "),e("p",[v._v("温度表：活跃度")]),v._v(" "),e("ul",[e("li",[v._v("当0，stop")]),v._v(" "),e("li",[v._v("当非常高，瞎几把乱选")]),v._v(" "),e("li",[v._v("当在中间，会根据价值和温度进行考虑，决定是否离开本地")])]),v._v(" "),e("p",[v._v("以此来避开局部最佳")]),v._v(" "),e("h5",{attrs:{id:"_2-3-4、genetic-algorithms"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-4、genetic-algorithms"}},[v._v("#")]),v._v(" 2.3.4、Genetic Algorithms")]),v._v(" "),e("blockquote",[e("p",[v._v("遗传算法")])]),v._v(" "),e("p",[v._v("简单举例：现在有两个完整的解决办法，但他们都有少数冲突，现在我们将两个办法砍开再重组")]),v._v(" "),e("h3",{attrs:{id:"_3、games"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3、games"}},[v._v("#")]),v._v(" 3、Games")]),v._v(" "),e("blockquote",[e("p",[v._v("Game Tree and Adversarial Serach")]),v._v(" "),e("p",[v._v("博弈树和对抗搜索")])]),v._v(" "),e("p",[v._v("games with ai")]),v._v(" "),e("ul",[e("li",[v._v("checkers（solved）")]),v._v(" "),e("li",[v._v("chess: deep blue")]),v._v(" "),e("li",[v._v("go")]),v._v(" "),e("li",[v._v("pacman")])]),v._v(" "),e("p",[v._v("types of games")]),v._v(" "),e("ul",[e("li",[v._v("确定性（围棋）或随机性（吃豆人）")]),v._v(" "),e("li",[v._v("玩家数量")]),v._v(" "),e("li",[v._v("零和博弈")]),v._v(" "),e("li",[v._v("信息是否公开（扑克、围棋）")])]),v._v(" "),e("p",[v._v("Many possible formalizations")]),v._v(" "),e("ul",[e("li",[v._v("states: S（start at s）")]),v._v(" "),e("li",[v._v("players: P（1...N）")]),v._v(" "),e("li",[v._v("Actions: A")]),v._v(" "),e("li",[v._v("Transition: Function: SxA -> S")]),v._v(" "),e("li",[v._v("Terminal Test: S -> {t, f}")]),v._v(" "),e("li",[v._v("Terminal Utilities: SxP -> R")])]),v._v(" "),e("p",[v._v("Solution for a player is a policy: S -> A")]),v._v(" "),e("blockquote",[e("p",[v._v("the point is: what action should be taken")])]),v._v(" "),e("p",[v._v("Zero-Sum Games")]),v._v(" "),e("ul",[e("li",[v._v("博弈双方争夺同一组资源（双人红警），一方资源的增加必然等于另一方资源的减少，是为零和，故双方不存在合作的可能")]),v._v(" "),e("li",[v._v("属于非合作博弈")])]),v._v(" "),e("p",[v._v("General Games")]),v._v(" "),e("ul",[e("li",[v._v("有同伴，如MC")]),v._v(" "),e("li",[v._v("属于合作博弈")])]),v._v(" "),e("p",[e("strong",[v._v("Adversarial Game Tree")])]),v._v(" "),e("ul",[e("li",[v._v("博弈树")])]),v._v(" "),e("p",[v._v("Single-Agent Trees")]),v._v(" "),e("ul",[e("li",[v._v("单策略树")]),v._v(" "),e("li",[v._v("Minimax Value：节点的值等于其子节点中的最大值")]),v._v(" "),e("li",[v._v("Minimum Value：节点的值等于其子结点中的最小值")])]),v._v(" "),e("h4",{attrs:{id:"_3-1、minimax-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1、minimax-search"}},[v._v("#")]),v._v(" 3.1、Minimax Search")]),v._v(" "),e("blockquote",[e("p",[v._v("井字棋：确定化的零和游戏")]),v._v(" "),e("p",[v._v("like DFS")])]),v._v(" "),e("p",[v._v("这常用于解决公开信息的零和问题：因为玩家回合制操作，在自己的回合要求MAX，在对方的回合要求MIN，Minimax算法正合他意")]),v._v(" "),e("p",[e("strong",[v._v("Tic-Tac-Toe Game Tree")])]),v._v(" "),e("p",[v._v("最小化节点和最大化节点一层一层层交替")]),v._v(" "),e("ul",[e("li",[v._v("最小化节点找到下一层节点的最小值为自己赋值")]),v._v(" "),e("li",[v._v("最大化节点找到下一层节点的最大值为自己赋值")]),v._v(" "),e("li",[v._v("这样的赋值是动态的，实时变化的，直到所有节点被访问")])]),v._v(" "),e("p",[v._v("最后将会根据根节点值选择相应叶子节点连接成结果路径")]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Strategy")]),v._v(" "),e("th",[v._v("Time")]),v._v(" "),e("th",[v._v("Space")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("Minimax")]),v._v(" "),e("td",[v._v("O(b^m)")]),v._v(" "),e("td",[v._v("O(bm)")])]),v._v(" "),e("tr",[e("td",[v._v("DFS")]),v._v(" "),e("td",[v._v("O(b^m)")]),v._v(" "),e("td",[v._v("O(bm)")])])])]),v._v(" "),e("p",[v._v("当b>35时，搜索整棵树变得不太可能，效率太低辽（Resource limit）")]),v._v(" "),e("h4",{attrs:{id:"_3-2、α-β-pruning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2、α-β-pruning"}},[v._v("#")]),v._v(" 3.2、α-β Pruning")]),v._v(" "),e("blockquote",[e("p",[v._v("阿尔法贝塔算法：博弈树剪枝的一种方式")])]),v._v(" "),e("p",[v._v("在"),e("code",[v._v("minimax search")]),v._v("的基础上，进行一些价值判断从而跳过一些节点")]),v._v(" "),e("p",[v._v("比如上层的最大化节点已经遍历到了一个最大值3，搜索当前最小化节点下层时访问到了一个值为3的节点，那么我们可以直接跳过当前这个最小化节点，因为它一定会返回一个"),e("code",[v._v("<=3")]),v._v("的最小值，而它总会小于等于上层最大化节点的当前值3，不会大于3，不会更新MAX的值")]),v._v(" "),e("p",[v._v("General configuration (MIN version)")]),v._v(" "),e("ul",[e("li",[v._v("computing the MIN-VALUE node n")]),v._v(" "),e("li",[v._v("looping over n's children")]),v._v(" "),e("li",[v._v("n's estimate of the childrens' min is dropping（丢给上一层判定）")]),v._v(" "),e("li",[v._v("who cares about n's value? MAX-VALUE node m, assume m's value is a")]),v._v(" "),e("li",[v._v("when n's estimate worse than a（cur <= a）, skip n and continue computing next MIN-VALUE node")])]),v._v(" "),e("p",[e("strong",[v._v("Alpha-Beta Pruning")])]),v._v(" "),e("blockquote",[e("p",[v._v("α-β修剪：减少minimax search的成本")])]),v._v(" "),e("ul",[e("li",[v._v("Alpha：MAX's best option on path to root")]),v._v(" "),e("li",[v._v("Beta：MIN's best option on path to root")])]),v._v(" "),e("p",[v._v("The algorithm like this")]),v._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("def")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("-")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n\tv "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" MIN_VALUE\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("for")]),v._v(" node n "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("in")]),v._v(" successor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n        v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("if")]),v._v(" v"),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v(">=")]),v._v(" b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n        a "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("def")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[v._v("min_value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n    v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" MAX_VALUE\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("for")]),v._v(" node n "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("in")]),v._v(" successor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n        v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("min")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n        "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("if")]),v._v(" v"),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("<=")]),v._v(" a"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n        b "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("min")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("b"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n")])]),v._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[v._v("1")]),e("br"),e("span",{staticClass:"line-number"},[v._v("2")]),e("br"),e("span",{staticClass:"line-number"},[v._v("3")]),e("br"),e("span",{staticClass:"line-number"},[v._v("4")]),e("br"),e("span",{staticClass:"line-number"},[v._v("5")]),e("br"),e("span",{staticClass:"line-number"},[v._v("6")]),e("br"),e("span",{staticClass:"line-number"},[v._v("7")]),e("br"),e("span",{staticClass:"line-number"},[v._v("8")]),e("br"),e("span",{staticClass:"line-number"},[v._v("9")]),e("br"),e("span",{staticClass:"line-number"},[v._v("10")]),e("br"),e("span",{staticClass:"line-number"},[v._v("11")]),e("br"),e("span",{staticClass:"line-number"},[v._v("12")]),e("br"),e("span",{staticClass:"line-number"},[v._v("13")]),e("br"),e("span",{staticClass:"line-number"},[v._v("14")]),e("br"),e("span",{staticClass:"line-number"},[v._v("15")]),e("br")])]),e("ul",[e("li",[e("p",[v._v("it could be very deep and expensive")])]),v._v(" "),e("li",[e("p",[v._v("good child ordering improves effectiveness of pruning")]),v._v(" "),e("p",[v._v("如第一次找计算量最小的子树对Top进行赋值")])])]),v._v(" "),e("p",[v._v("这样仍然会消耗很多资源，顶多开个根")]),v._v(" "),e("p",[e("strong",[v._v("Depth-Limited Minimax Search")])]),v._v(" "),e("p",[v._v("于是我们选择使用深度受限的Minimax Search以节省资源，将有限最大深度看作叶子节点，这样会不太准，只是近似值（not solved）")]),v._v(" "),e("ul",[e("li",[v._v("如下国际象棋，只往后推8步，而不是63步")])]),v._v(" "),e("p",[v._v("a danger of replanning agents：")]),v._v(" "),e("p",[v._v("当子节点价值一样，父节点将会左右摇摆")]),v._v(" "),e("p",[v._v("这取决于你的评估功能，当他不够好时，将会出现很奇怪的行为（如左右摇摆）")]),v._v(" "),e("p",[v._v("设计一个细致的评估机制，对各节点适量加权，"),e("code",[v._v("even worse always exist")])]),v._v(" "),e("p",[v._v("有限深度的影响")]),v._v(" "),e("ul",[e("li",[v._v("当深度过少，很难找到合适的解决方案")]),v._v(" "),e("li",[v._v("深度增多会导致计算量增大，但更可能赢得游戏")])]),v._v(" "),e("p",[v._v("当算力有限时，评估功能的准确性和搜索深度决定了是否能找到更有前途的解决方案")]),v._v(" "),e("ul",[e("li",[v._v("Evaluation and Depth matters")])]),v._v(" "),e("h4",{attrs:{id:"_3-3、expectimax-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3、expectimax-search"}},[v._v("#")]),v._v(" 3.3、Expectimax Search")]),v._v(" "),e("blockquote",[e("p",[v._v("we always have uncertain outcomes")]),v._v(" "),e("p",[v._v("最大期望搜索")]),v._v(" "),e("p",[v._v("机会博弈")])]),v._v(" "),e("p",[v._v("在"),e("code",[v._v("minimax search")]),v._v("和"),e("code",[v._v("α-β purning")]),v._v("时，我们对于下层节点的选择都是随机的，但若优先选择到更优的元素，可以有效对无用数据进行修剪")]),v._v(" "),e("p",[v._v("根据概率对各节点进行建模（modeling），但是实际上这种选择是不确定的")]),v._v(" "),e("p",[v._v("Expectimax Search")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Max nodes as in minimax search")]),v._v(" "),e("p",[v._v("最大节点和最小最大搜索中的最大节点保持一致")])]),v._v(" "),e("li",[e("p",[v._v("Chance nodes are like min nodes but outcome is uncertain")]),v._v(" "),e("p",[v._v("机会节点像最小节点，但他的结果是不确定的，是估值、随机的")])]),v._v(" "),e("li",[e("p",[v._v("Calculate their utilities（均值）")])]),v._v(" "),e("li",[e("p",[v._v("Take weighted average（expectation） of children")])])]),v._v(" "),e("p",[v._v("计算最佳游戏下的平均得分，选择均分最高的节点拓展，有一种希望得分为均分的想法，以期望优先（均分=选择概率*节点总值）")]),v._v(" "),e("ul",[e("li",[v._v("当没有明确概率时，默认平均")])]),v._v(" "),e("p",[v._v("需要注意的是，这里和最大最小搜索的是同一颗博弈树，只是算法不同")]),v._v(" "),e("p",[v._v("举个例子：当你失血过多，是爬出去求救还是立刻自杀")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("若是minimax算法，他将选择下一步价值最高的节点，即立即自杀以减少痛苦")])]),v._v(" "),e("li",[e("p",[v._v("若是expectimax算法，他将评估爬出去求救的平均价值（即被救还是痛苦死去）和立刻死去的价值，若求救的均值大于立刻死去，那么他就会选择求救")]),v._v(" "),e("p",[v._v("但注意，这里是否被救是不确定的，有可能陷入更悲观的结局")])])]),v._v(" "),e("p",[v._v("Expectimax Search")]),v._v(" "),e("div",{staticClass:"language-python line-numbers-mode"},[e("pre",{pre:!0,attrs:{class:"language-python"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("def")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[v._v("value")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("if")]),v._v(" the "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("is")]),v._v(" a terminal state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" the state's utility\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("if")]),v._v(" the "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("next")]),v._v(" agent "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("is")]),v._v(" MAX"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("-")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("if")]),v._v(" the "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("next")]),v._v(" agent "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("is")]),v._v(" EXP"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" exp"),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("-")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    \n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("def")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("-")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n    v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" MIN_VALUE\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("for")]),v._v(" node "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("in")]),v._v(" successor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n        v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token builtin"}},[v._v("max")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("v"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(",")]),v._v(" value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("node"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("def")]),v._v(" exp"),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("-")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("state"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n    v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[v._v("0")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("for")]),v._v(" node "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("in")]),v._v(" successor"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(":")]),v._v("\n        p "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("=")]),v._v(" probability"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("node"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n        v "),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("+=")]),v._v(" p"),e("span",{pre:!0,attrs:{class:"token operator"}},[v._v("*")]),v._v("value"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v("(")]),v._v("node"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[v._v(")")]),v._v("\n    "),e("span",{pre:!0,attrs:{class:"token keyword"}},[v._v("return")]),v._v(" v\n")])]),v._v(" "),e("div",{staticClass:"line-numbers-wrapper"},[e("span",{staticClass:"line-number"},[v._v("1")]),e("br"),e("span",{staticClass:"line-number"},[v._v("2")]),e("br"),e("span",{staticClass:"line-number"},[v._v("3")]),e("br"),e("span",{staticClass:"line-number"},[v._v("4")]),e("br"),e("span",{staticClass:"line-number"},[v._v("5")]),e("br"),e("span",{staticClass:"line-number"},[v._v("6")]),e("br"),e("span",{staticClass:"line-number"},[v._v("7")]),e("br"),e("span",{staticClass:"line-number"},[v._v("8")]),e("br"),e("span",{staticClass:"line-number"},[v._v("9")]),e("br"),e("span",{staticClass:"line-number"},[v._v("10")]),e("br"),e("span",{staticClass:"line-number"},[v._v("11")]),e("br"),e("span",{staticClass:"line-number"},[v._v("12")]),e("br"),e("span",{staticClass:"line-number"},[v._v("13")]),e("br"),e("span",{staticClass:"line-number"},[v._v("14")]),e("br"),e("span",{staticClass:"line-number"},[v._v("15")]),e("br"),e("span",{staticClass:"line-number"},[v._v("16")]),e("br"),e("span",{staticClass:"line-number"},[v._v("17")]),e("br"),e("span",{staticClass:"line-number"},[v._v("18")]),e("br"),e("span",{staticClass:"line-number"},[v._v("19")]),e("br")])]),e("p",[e("strong",[v._v("Expectimax Pruning")])]),v._v(" "),e("p",[v._v("最大期望剪枝")]),v._v(" "),e("p",[v._v("由于总有可能存在极端值使均值失衡，最大期望算法不能剪枝")]),v._v(" "),e("p",[e("strong",[v._v("Depth-Limited Expectimax")])]),v._v(" "),e("p",[v._v("深度受限的最大期望搜索")]),v._v(" "),e("p",[v._v("可以通过机器学习更准确的预测期望值")]),v._v(" "),e("p",[v._v("其核心思想是减少层数的遍历通过均值估计得到一个顶部的近似值")]),v._v(" "),e("p",[e("strong",[v._v("Probabilities")])]),v._v(" "),e("p",[v._v("根据"),e("code",[v._v("world model")]),v._v("，针对不同的情况分配概率")]),v._v(" "),e("p",[v._v("注意概率是动态变化的，当你获得的信息越多，概率越准确，估值也越准确")]),v._v(" "),e("p",[v._v("Expectations：期望值 —— 分布概率加权")]),v._v(" "),e("ul",[e("li",[v._v("如三种概率分别对应三种耗时，那么期望值"),e("code",[v._v("expectations=0.2x20+0.3x15+0.5x30=24.5")])])]),v._v(" "),e("p",[v._v("在Expectimax Search中维护一个概率模型，计算各节点的发生概率，通过简单或复杂的计算或随机确定概率")]),v._v(" "),e("ul",[e("li",[v._v("不一定对，只是概率")])]),v._v(" "),e("p",[e("strong",[v._v("Informed Probabilities")])]),v._v(" "),e("p",[v._v("当存在一个竞争对手，进行概率计算时，要同时对自己和对方的概率进行建模，这将使概率计算十分昂贵")]),v._v(" "),e("p",[v._v("因为各自都有一棵博弈树，当搜索算法不同时，二者树也不尽相同，为了相互匹配将带来更多计算")]),v._v(" "),e("p",[v._v("这也意味着在竞争时，你不得不越来越接近你的对手，若对手搜索到了100层，你必须做出相应回应，至少近似的搜索到100层，并希望估值足够好")]),v._v(" "),e("p",[e("strong",[v._v("The dangers of optimism and pessimism")])]),v._v(" "),e("p",[v._v("乐观和悲观的危险")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("assuming chance when the world is adversarial（对抗的）")]),v._v(" "),e("p",[v._v("太乐观了，太贪了，有时很有效，但不安全")])]),v._v(" "),e("li",[e("p",[v._v("assuming the worst case when it's not likely")]),v._v(" "),e("p",[v._v("太怂了，或许最后一个豆就在眼前，鬼在较远处，它也会优先选择远离鬼的方向扩展")])])]),v._v(" "),e("p",[v._v("分别对应了"),e("code",[v._v("Expectimax Search")]),v._v("和"),e("code",[v._v("Minimax Search")])]),v._v(" "),e("p",[v._v("Assumptions vs. Reality")]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Packman\\Ghost")]),v._v(" "),e("th",[v._v("Adversarial")]),v._v(" "),e("th",[v._v("Random")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("Minimax")]),v._v(" "),e("td",[v._v("good")]),v._v(" "),e("td",[v._v("always waste")])]),v._v(" "),e("tr",[e("td",[v._v("Expectimax")]),v._v(" "),e("td",[v._v("always get eaten")]),v._v(" "),e("td",[v._v("good")])])])]),v._v(" "),e("p",[e("strong",[v._v("Other Type Game")])]),v._v(" "),e("blockquote",[e("p",[v._v("Mixed Layer Types")])]),v._v(" "),e("p",[v._v("Backgammon")]),v._v(" "),e("p",[v._v("每步二十个扩展")]),v._v(" "),e("p",[v._v("深度二搜索 ——> build a world champion")]),v._v(" "),e("p",[v._v("Multi-Agent Utilities")]),v._v(" "),e("h4",{attrs:{id:"_3-4、utilities"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4、utilities"}},[v._v("#")]),v._v(" 3.4、Utilities")]),v._v(" "),e("blockquote",[e("p",[v._v("效用")])]),v._v(" "),e("p",[v._v("Maximum Expected Utility")]),v._v(" "),e("blockquote",[e("p",[v._v("为什么不用minimax，因为他总会太过敏感，做出不合理的选择")])]),v._v(" "),e("p",[v._v("搜索方式将决定我们玩游戏的方式，即如何选择下一步的方法")]),v._v(" "),e("p",[v._v("奖品和彩票（actual utility）")]),v._v(" "),e("ul",[e("li",[v._v("Prizes：确定的")]),v._v(" "),e("li",[v._v("Lotteries：不确定的（uncertain prizes）")])]),v._v(" "),e("p",[e("strong",[v._v("Rational")])]),v._v(" "),e("ul",[e("li",[e("p",[v._v("orderability（明确性）")]),v._v(" "),e("p",[v._v("a>b&&b>a ——> a==b")])]),v._v(" "),e("li",[e("p",[v._v("transitivity（传递性）")]),v._v(" "),e("p",[v._v("a>b&&b>c ——> a>c")])]),v._v(" "),e("li",[e("p",[v._v("continuity（连续性）")]),v._v(" "),e("p",[v._v("a>b>c ——> 你需要在a和c之间抽奖")])]),v._v(" "),e("li",[e("p",[v._v("substitutability（可替代性）")]),v._v(" "),e("p",[v._v("a~b ——> 那么所有涉及a的判断可以用b替代")])]),v._v(" "),e("li",[e("p",[v._v("monotonicty（单调性）")]),v._v(" "),e("p",[v._v("a>b, P(a)>P(b) ——> select a")])])]),v._v(" "),e("p",[v._v("MEU Principle")]),v._v(" "),e("p",[v._v("偏好和价值")]),v._v(" "),e("p",[e("strong",[v._v("Human Utility")])]),v._v(" "),e("p",[v._v("极端情况（extremes）")]),v._v(" "),e("p",[v._v("效用函数")]),v._v(" "),e("p",[v._v("平均操作使行为如一（expectimax）")]),v._v(" "),e("ul",[e("li",[v._v("Money")]),v._v(" "),e("li",[v._v("Lottery")]),v._v(" "),e("li",[v._v("Prizes")])]),v._v(" "),e("p",[v._v("Human Rationality?")]),v._v(" "),e("blockquote",[e("p",[v._v("马尔可夫决策和强化学习")])]),v._v(" "),e("h3",{attrs:{id:"_4、mdps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4、mdps"}},[v._v("#")]),v._v(" 4、MDPs")]),v._v(" "),e("blockquote",[e("p",[v._v("Markov Decision Processes")]),v._v(" "),e("p",[v._v("马尔可夫决定过程")])]),v._v(" "),e("h4",{attrs:{id:"_4-1、what-is-mdps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-1、what-is-mdps"}},[v._v("#")]),v._v(" 4.1、What is MDPs")]),v._v(" "),e("p",[v._v("进程：对搜索的概括")]),v._v(" "),e("p",[v._v("计算可能的结果")]),v._v(" "),e("p",[v._v("在"),e("code",[v._v("GridWorld")]),v._v("中，你决定向北走（因为这是最佳策略），但可能会执行失败（撞墙）")]),v._v(" "),e("p",[v._v("MDP：Reward ——> 结果")]),v._v(" "),e("ul",[e("li",[v._v("happy reward")]),v._v(" "),e("li",[v._v("bad reward")])]),v._v(" "),e("p",[v._v("目标很松散 ——>为了最大化奖励的总和")]),v._v(" "),e("p",[v._v("An MDP is defined by")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("a set of states s")])]),v._v(" "),e("li",[e("p",[v._v("a set of actions a")])]),v._v(" "),e("li",[e("p",[v._v("a transition function T(s, a, s')")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Probability that a from s leads to s', called P(s'| s, a)")]),v._v(" "),e("p",[v._v("在s状态执行a行为到达s'的代价")])]),v._v(" "),e("li",[e("p",[v._v("Also called the model or the dynamics")]),v._v(" "),e("p",[v._v("不同于搜索，这个后继函数有很多个，如在每个地点都可以向东南西北移动")])])])]),v._v(" "),e("li",[e("p",[v._v("a reward function R(s, a, s')")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("sometimes just R(s) or R(s')")]),v._v(" "),e("p",[v._v("奖惩制度，有时只取决起点或终点")])])])]),v._v(" "),e("li",[e("p",[v._v("a start state")])]),v._v(" "),e("li",[e("p",[v._v("maybe a terminal state")])])]),v._v(" "),e("p",[v._v("MDPs是不确定性搜索问题")]),v._v(" "),e("ul",[e("li",[v._v("强化学习的基础")])]),v._v(" "),e("p",[v._v("expectimax（最大期望算法）算法可以MDP问题")]),v._v(" "),e("p",[v._v("action outcomes depend on")]),v._v(" "),e("ul",[e("li",[v._v("未来要到达的状态")]),v._v(" "),e("li",[v._v("你要执行的行动")])]),v._v(" "),e("p",[v._v("MDPs适合嘈杂的世界")]),v._v(" "),e("p",[e("strong",[v._v("Grid World")])]),v._v(" "),e("p",[v._v("Policy：策略")]),v._v(" "),e("ul",[e("li",[v._v("通过状态告诉你动作的功能")]),v._v(" "),e("li",[v._v("如在地图上每个点标好你该往哪个方向走")])]),v._v(" "),e("p",[v._v("optimal policy：最优策略")]),v._v(" "),e("ul",[e("li",[v._v("或许存在很多等效策略")])]),v._v(" "),e("p",[v._v("competition")]),v._v(" "),e("ul",[e("li",[v._v("移动奖励（负0.1）是那么微不足道而不值得冒险去坑附近")]),v._v(" "),e("li",[v._v("宁愿什么都不做，也不愿犯错")])]),v._v(" "),e("p",[v._v("当移动代价变得更大，策略将更倾向与冒险在坑附近")]),v._v(" "),e("p",[v._v("当更更大时，甚至有可能直接跳坑而避免移动花销")]),v._v(" "),e("p",[e("strong",[v._v("Racing")])]),v._v(" "),e("p",[v._v("states：")]),v._v(" "),e("ul",[e("li",[v._v("cool")]),v._v(" "),e("li",[v._v("warm")]),v._v(" "),e("li",[v._v("overheated：risk the danger of breaking")])]),v._v(" "),e("p",[v._v("跟据当前的温度决定是加速还是减速")]),v._v(" "),e("p",[e("strong",[v._v("Racing Search Tree")])]),v._v(" "),e("blockquote",[e("p",[v._v("tool：epectimax search")])]),v._v(" "),e("p",[v._v("actions：")]),v._v(" "),e("ul",[e("li",[v._v("slower")]),v._v(" "),e("li",[v._v("faster")])]),v._v(" "),e("p",[v._v("state：")]),v._v(" "),e("ul",[e("li",[v._v("warm")]),v._v(" "),e("li",[v._v("cool")]),v._v(" "),e("li",[v._v("over heated")])]),v._v(" "),e("p",[v._v("这棵树是无限的")]),v._v(" "),e("ul",[e("li",[v._v("Q state：选择了但还没行动的过度状态")])]),v._v(" "),e("p",[e("strong",[v._v("Utilities of Sequences")])]),v._v(" "),e("p",[v._v("实用程序的选择顺序")]),v._v(" "),e("ul",[e("li",[v._v("more or less")]),v._v(" "),e("li",[v._v("now or later")])]),v._v(" "),e("p",[v._v("隐含的权衡")]),v._v(" "),e("p",[e("strong",[v._v("discount")])]),v._v(" "),e("p",[v._v("对奖励的贬值，对晚来的价值施以惩罚，如每走一步，未得到的价值便腐朽0.8，0.8便是折扣")]),v._v(" "),e("p",[v._v("当折扣越大，即"),e("code",[v._v("λ")]),v._v("越小，agent将变得越贪婪，越在意眼前的价值，而不是以后获得更大的利益")]),v._v(" "),e("p",[e("strong",[v._v("Preferences")])]),v._v(" "),e("p",[v._v("假设偏好是固定的")]),v._v(" "),e("p",[v._v("two ways to define utilities")]),v._v(" "),e("ul",[e("li",[v._v("additive utility")]),v._v(" "),e("li",[v._v("discounted utility")])]),v._v(" "),e("p",[v._v("如何处理无限的问题")]),v._v(" "),e("ul",[e("li",[v._v("Finite horizen：similar to depth-limited search，即限定树的深度")]),v._v(" "),e("li",[v._v("Discounting：价值总是贬值，将无限接近于0")]),v._v(" "),e("li",[v._v("Absorbing state：使用一系列终止状态，即")])]),v._v(" "),e("p",[v._v("Markov decision processes：")]),v._v(" "),e("ul",[e("li",[v._v("状态集")]),v._v(" "),e("li",[v._v("初始状态")]),v._v(" "),e("li",[v._v("行为集")]),v._v(" "),e("li",[v._v("过渡函数：提供的是概率")]),v._v(" "),e("li",[v._v("奖惩机制")])]),v._v(" "),e("p",[v._v("它的输出是每个state上对应的action，他实际上并没有真正在试错，而是去给每个状态分配最佳的行动，这就是MDP")]),v._v(" "),e("h4",{attrs:{id:"_4-2、solving-mdps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2、solving-mdps"}},[v._v("#")]),v._v(" 4.2、Solving MDPs")]),v._v(" "),e("p",[v._v("Quantities：")]),v._v(" "),e("ul",[e("li",[v._v("Policy：map of states of actions")]),v._v(" "),e("li",[v._v("Utility：sum of discounted reward")]),v._v(" "),e("li",[v._v("Values：expected future utility from a state（max node）")]),v._v(" "),e("li",[v._v("Q Values：expected future from a q-state（chance node）")])]),v._v(" "),e("p",[v._v("Optimal Quantities")]),v._v(" "),e("ul",[e("li",[v._v("V(s)*：状态的期望值（或许是平均值）")]),v._v(" "),e("li",[v._v("Q*(s, a)：在状态s执行动作a后起的最佳作用")]),v._v(" "),e("li",[v._v("P*(s)：当前状态的最佳策略（算法产出）")])]),v._v(" "),e("p",[v._v("expectimax search可以解决这一问题，估算价值，选出最大价值，赋值")]),v._v(" "),e("p",[v._v("考虑一下其他的算法")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("V*(s) = maxQ*(s, a)")])]),v._v(" "),e("p",[v._v("虽然Q*(s, a)还不知道怎么算")])]),v._v(" "),e("li",[e("p",[e("code",[v._v("Q*(s, a) = avg(sum(R(s, a, s') + λV*(s'))")])]),v._v(" "),e("p",[v._v("这是一个递归的定义，因为你并不知道V*(s')直到搜索到终点")])])]),v._v(" "),e("p",[v._v("此之谓贝尔曼方程：Bellman Equations")]),v._v(" "),e("ul",[e("li",[v._v("take correct first action")]),v._v(" "),e("li",[v._v("kepp being optimal")])]),v._v(" "),e("p",[v._v("回顾一下Racing Search Tree")]),v._v(" "),e("p",[v._v("他是无限的，并且只有三种状态，如果用expectimax search，将会有指数级的重复工作（子树）")]),v._v(" "),e("h5",{attrs:{id:"_4-2-1、value-iteration"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-1、value-iteration"}},[v._v("#")]),v._v(" 4.2.1、Value Iteration")]),v._v(" "),e("p",[v._v("价值迭代算法")]),v._v(" "),e("ul",[e("li",[v._v("from the bottom（deep enough）, recur the top")]),v._v(" "),e("li",[e("code",[v._v("V*(s) = maxΣT(s,a,s')[R(s,a,s') + λV*(s')]")])])]),v._v(" "),e("p",[v._v("利用贝尔曼方程确实可以搜索到底部并且递归回顶部，在这个递归过程中，各节点的值是不断更新的，且更加准确，直到保持稳定，即递归完毕")]),v._v(" "),e("ul",[e("li",[v._v("这个收敛的过程称作"),e("code",[v._v("bellman update")])])]),v._v(" "),e("p",[e("strong",[v._v("Computing Time-Limited Values")])]),v._v(" "),e("p",[v._v("对于一颗无限树，采用时间限制其递归深度，令V*(s)尽可能准确")]),v._v(" "),e("p",[v._v("因为条件有限，我们无法完整进行贝尔曼算法，即只能尽可能的接近V*(s)")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("Vk(s) = avg(sum(R(s, a, s') + λVk(s')))")])])]),v._v(" "),e("p",[v._v("其中"),e("code",[v._v("Vk(s)、Vk(s')")]),v._v("都取其均值")]),v._v(" "),e("ul",[e("li",[v._v("take average")]),v._v(" "),e("li",[v._v("像一个单层的expectimax搜索，但不同的是，他会由于递归深度的增加不断调整Vk值")])]),v._v(" "),e("p",[e("strong",[v._v("Convergence")])]),v._v(" "),e("p",[v._v("VK compute")]),v._v(" "),e("p",[v._v("一个k层树和一个k+1层树")]),v._v(" "),e("p",[v._v("由于搜索深度增加，对于未来某节点的折扣也增加，也就是说越往后对总值的影响应是越小，细微调整")]),v._v(" "),e("p",[v._v("当discount>=1，没有趋同保证")]),v._v(" "),e("h5",{attrs:{id:"_4-2-2、policy-evaluation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-2、policy-evaluation"}},[v._v("#")]),v._v(" 4.2.2、Policy Evaluation")]),v._v(" "),e("p",[v._v("策略评估方法")]),v._v(" "),e("h6",{attrs:{id:"_1fixed-policies"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1fixed-policies"}},[v._v("#")]),v._v(" ①fixed Policies")]),v._v(" "),e("p",[v._v("固定的策略")]),v._v(" "),e("ul",[e("li",[v._v("do the optimal action")]),v._v(" "),e("li",[v._v("do what Pi says")]),v._v(" "),e("li",[v._v("easier than the optimal")])]),v._v(" "),e("p",[v._v("假设你的固定策略选出的后继节点是最佳的")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("VΠ(s) = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]")])])]),v._v(" "),e("p",[v._v("固定策略例如：一直向右走；一直向前走")]),v._v(" "),e("p",[v._v("列举所有策略，评估所有策略，选择得分最高的策略")]),v._v(" "),e("h6",{attrs:{id:"_2policy-evaluation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2policy-evaluation"}},[v._v("#")]),v._v(" ②policy evaluation")]),v._v(" "),e("p",[v._v("输入一个策略，执行策略，得到该策略的值向量")]),v._v(" "),e("p",[e("code",[v._v("VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]")])]),v._v(" "),e("h6",{attrs:{id:"_3policy-extraction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3policy-extraction"}},[v._v("#")]),v._v(" ③Policy Extraction")]),v._v(" "),e("p",[v._v("即使当找到了相邻的最佳值，仍然要做一次expectimax去找到导致这个最佳值的行动")]),v._v(" "),e("ul",[e("li",[v._v("从值中找出行动，以更新策略")])]),v._v(" "),e("p",[v._v("价值驱动决策")]),v._v(" "),e("p",[v._v("Computing Actions from Q-Values")]),v._v(" "),e("h5",{attrs:{id:"_4-2-3、policy-iteration"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-3、policy-iteration"}},[v._v("#")]),v._v(" 4.2.3、Policy Iteration")]),v._v(" "),e("p",[v._v("价值迭代的问题")]),v._v(" "),e("ul",[e("li",[v._v("每次迭代将会耗费"),e("code",[v._v("O(s^2*A)")]),v._v("，这很慢")]),v._v(" "),e("li",[v._v("每个状态的最大值很少改变，这意味着做了很多低效工作")])]),v._v(" "),e("p",[v._v("正确策略下的无用选择 ——> 错误的策略试错")]),v._v(" "),e("p",[v._v("我们采用策略迭代")]),v._v(" "),e("ul",[e("li",[v._v("首先选择一些策略，并执行他们，估算状态价值")]),v._v(" "),e("li",[v._v("改善你的策略，再次考虑之前的行动，重复估值")]),v._v(" "),e("li",[v._v("直到策略收敛")])]),v._v(" "),e("p",[v._v("可以证明它是最佳且收敛的，并且在很多情况比价值迭代收敛得更快")]),v._v(" "),e("p",[v._v("VΠ是由当前策略得到的当前“最佳值”")]),v._v(" "),e("p",[e("code",[v._v("VΠ = ΣT(s,Π(s),s')[R(s,Π(s),s') + λVΠ(s')]")])]),v._v(" "),e("p",[v._v("根据这个当前最佳值，更新上一步的策略，比如我上一步策略原来是往北走，但这个最佳值得往东走，那么我更新上一步的策略为向东走")]),v._v(" "),e("ul",[e("li",[v._v("MDPs本质上便是找到每步的最佳策略，值迭代同时考虑策略和价值，在每步做出最佳选择；策略迭代通过值去找到更优的策略")]),v._v(" "),e("li",[v._v("二者都是迭代，从叶子回溯到顶部")])]),v._v(" "),e("p",[v._v("通常根据最后值的变化来确定是否已经收敛")]),v._v(" "),e("h4",{attrs:{id:"_4-3、summary"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_4-3、summary"}},[v._v("#")]),v._v(" 4.3、Summary")]),v._v(" "),e("ul",[e("li",[v._v("compute optimal values：both can")]),v._v(" "),e("li",[v._v("compute values for partivular policy：policy evaluation（策略评估）")]),v._v(" "),e("li",[v._v("turn your values into policy：use policy extraction（策略抽取）")])]),v._v(" "),e("p",[v._v("通常Policy Iteration是policy evaluation和policy improvement交替执行直到收敛")]),v._v(" "),e("p",[v._v("Value Iteration是寻找Optimal value function和执行一次policy extraction")]),v._v(" "),e("ul",[e("li",[v._v("均属于动态规划算法")])]),v._v(" "),e("p",[e("strong",[v._v("Double-Bandit MDP")])]),v._v(" "),e("p",[v._v("两台老虎机，一台（blue）拉一次给一块钱；另一台（red）拉一次给0元或2元。共拉一百次")]),v._v(" "),e("p",[v._v("更优的策略？")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("red one")]),v._v("获得2元的概率为0.75")])]),v._v(" "),e("p",[v._v("平均上")]),v._v(" "),e("ul",[e("li",[v._v("blue：100元")]),v._v(" "),e("li",[v._v("red：150元")])]),v._v(" "),e("p",[v._v("当获得2元的概率未知，尝试red one去获得信息")]),v._v(" "),e("p",[v._v("core of reinfocement Learning：exploraton")]),v._v(" "),e("p",[v._v("只能探索才能获取更多信息")]),v._v(" "),e("p",[v._v("pay for the infomation and get return")]),v._v(" "),e("p",[v._v("甚至不需要MDP算法，只需要不断探索和基本的数学直觉，试出概率")]),v._v(" "),e("h3",{attrs:{id:"_5、rl"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5、rl"}},[v._v("#")]),v._v(" 5、RL")]),v._v(" "),e("blockquote",[e("p",[v._v("Reinforcement learning：强化学习")]),v._v(" "),e("p",[v._v("It's about how to learn behaviors")])]),v._v(" "),e("ul",[e("li",[v._v("Agent —actions—> Environment")]),v._v(" "),e("li",[v._v("Environment —state/reward—> Agent")])]),v._v(" "),e("p",[v._v("Agent和Environment都是动态变化的")]),v._v(" "),e("p",[v._v("Basic idea：")]),v._v(" "),e("ul",[e("li",[v._v("agent接收奖惩反馈")]),v._v(" "),e("li",[v._v("奖惩函数决定agent的效用")]),v._v(" "),e("li",[v._v("为了最大化奖励，必须去学习最优行动")]),v._v(" "),e("li",[v._v("所有的学习基于观察样例后的结果")])]),v._v(" "),e("p",[v._v("learning rather than plan")]),v._v(" "),e("p",[v._v("Examples：")]),v._v(" "),e("ul",[e("li",[v._v("Robot dog learning to Walk")]),v._v(" "),e("li",[v._v("Snake rebot sidewingding（爬墙）")])]),v._v(" "),e("p",[v._v("因为真实世界的规则并不是确定的，难以建模，这时让程序根据概率学习正确的行为显得更加高效")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Toddler Robot（幼儿机器人）")]),v._v(" "),e("p",[v._v("know how to stand after fall down")])])]),v._v(" "),e("p",[v._v("机器学习的最开始，他是不知道怎么做的，只是来回摆动，因为他不知道怎么获取奖励，于是开始瞎几把试，当偶然获取奖励后，他将根据奖惩制度完善自己的行动策略，从而行动得更加高效")]),v._v(" "),e("p",[v._v("Still assume a Markov decision process")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("a set of states")])]),v._v(" "),e("li",[e("p",[v._v("a set of actions")])]),v._v(" "),e("li",[e("p",[v._v("a model T(s,a,s')")]),v._v(" "),e("p",[v._v("原为 a successor function T(s,a,s')")])]),v._v(" "),e("li",[e("p",[v._v("a reward function R(s,a,s')")])])]),v._v(" "),e("p",[v._v("Still looking for a policy")]),v._v(" "),e("p",[v._v("The defference：We don't know T or R")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("不知道哪个状态是好的或哪个动作是好的")]),v._v(" "),e("p",[v._v("就像那个老虎机不知道掉落概率")])]),v._v(" "),e("li",[e("p",[v._v("必须真正去行动和访问状态去学习，去获取必要信息")])])]),v._v(" "),e("p",[v._v("Offline（MDPs） vs. Online（RL）")]),v._v(" "),e("ul",[e("li",[v._v("Offline Solution")]),v._v(" "),e("li",[v._v("Online Learning")])]),v._v(" "),e("h4",{attrs:{id:"_5-1、model-based-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-1、model-based-learning"}},[v._v("#")]),v._v(" 5.1、Model-Based Learning")]),v._v(" "),e("p",[v._v("Basic idea：")]),v._v(" "),e("ul",[e("li",[v._v("learn an approximate model based on experiences")]),v._v(" "),e("li",[v._v("solve for values as if the learned model were correct")])]),v._v(" "),e("p",[v._v("现根据经验构建模型，再使用问题求解方法去计算当前模型")]),v._v(" "),e("p",[v._v("就像一个CSP问题我们不知道联系，得先建立相邻状态联系")]),v._v(" "),e("p",[v._v("step1：learn empirical MDP model")]),v._v(" "),e("ul",[e("li",[v._v("为每个状态和动作做产出（outcomes）统计")]),v._v(" "),e("li",[v._v("常态化评估函数T(s,a,s')")]),v._v(" "),e("li",[v._v("每当经历"),e("code",[v._v("s—a—>s'")]),v._v("时计算回报函数R(s,a,s')")])]),v._v(" "),e("p",[v._v("step2：solve the learned MDP（近似的MDP问题）")]),v._v(" "),e("ul",[e("li",[v._v("use value iteration")]),v._v(" "),e("li",[v._v("use policy iteration")]),v._v(" "),e("li",[v._v("......")])]),v._v(" "),e("p",[v._v("T和R是未知的，但状态空间和行为空间被分配了，要做的就是收集更多数据，动态改善你的模型，估计T和R函数")]),v._v(" "),e("p",[v._v("where the reward function come from")]),v._v(" "),e("ul",[e("li",[v._v("depend on the human designer")])]),v._v(" "),e("p",[v._v("how to calculate T function")]),v._v(" "),e("ul",[e("li",[v._v("in a simple example, may just looking at the frequencies（频率）")])]),v._v(" "),e("p",[v._v("计算概率权值：E（概率x值）")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Known P(A)：E(A) = ΣP(a)*a")])]),v._v(" "),e("li",[e("p",[v._v("Unknown P(A)")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Model Based：E(A) = avg(sum(P(a)*a))")]),v._v(" "),e("p",[v._v("以某种策略重新计算概率")])]),v._v(" "),e("li",[e("p",[v._v("Model free：E(A) = (1/N)*sum(a)")]),v._v(" "),e("p",[v._v("我们认为各种可能概率是相等的，因为尚未总结出规律")])]),v._v(" "),e("li",[e("p",[v._v("二者区别在于是否按概率加权计算均值")])])])])]),v._v(" "),e("h4",{attrs:{id:"_5-2、model-free-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2、model-free-learning"}},[v._v("#")]),v._v(" 5.2、Model-Free Learning")]),v._v(" "),e("h5",{attrs:{id:"_5-2-1、value-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-1、value-learning"}},[v._v("#")]),v._v(" 5.2.1、Value Learning")]),v._v(" "),e("blockquote",[e("p",[v._v("Passive Reinforcement Learning")]),v._v(" "),e("p",[v._v("我们不担心如何在世界模型中行动，只是观察行动并视图估计此代理的状态值")])]),v._v(" "),e("p",[v._v("Simplified task：policy evaluation")]),v._v(" "),e("ul",[e("li",[v._v("input：a fixed policy（遵循某一策略）")]),v._v(" "),e("li",[v._v("don't know T(s,a,s')")]),v._v(" "),e("li",[v._v("don't know R(s,a,s')")]),v._v(" "),e("li",[v._v("goal：learn the state values")])]),v._v(" "),e("p",[v._v("Direct & Indirect Evaluation")]),v._v(" "),e("blockquote",[e("p",[v._v("直接估值和间接估值")])]),v._v(" "),e("p",[v._v("直接估值平均观察到的样本值，直接问这一步会有多少"),e("code",[v._v("reward")]),v._v("，仅仅依据实验出的结果的各状态值")]),v._v(" "),e("p",[v._v("如直接对于个节点的可能取值求均值作为其状态值，如对C节点使用四次策略")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("C向D -1，D退出+10")]),v._v(" "),e("p",[v._v("C向D -1，D退出+10")]),v._v(" "),e("p",[v._v("C向D -1，D退出+10")]),v._v(" "),e("p",[v._v("C向A -1，A退出-10")]),v._v(" "),e("p",[v._v("那么取均值则为"),e("code",[v._v("(9+9+9-11)/4=4")])])])]),v._v(" "),e("p",[v._v("不需要对T/R做任何事，求均值就行了，只关注值；这不能达到超精确，但随着数据增加总会愈加接近")]),v._v(" "),e("p",[v._v("要做的事很明确：")]),v._v(" "),e("ul",[e("li",[v._v("选择一个节点")]),v._v(" "),e("li",[v._v("多次使用策略进行扩展")]),v._v(" "),e("li",[v._v("对扩展结果进行分析取均")]),v._v(" "),e("li",[v._v("对该节点赋值得到"),e("code",[v._v("V(s)")])]),v._v(" "),e("li",[v._v("更新值和策略")])]),v._v(" "),e("p",[v._v("这一过程始终没用到T/R函数")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("VΠ(s) <-- (1/n)Σsample(i)")])])]),v._v(" "),e("p",[v._v("注意这里所有的"),e("code",[v._v("V(s')")]),v._v("都应乘上一个"),e("code",[v._v("λ(<=1)")]),v._v("作为时间惩罚（贬值）")]),v._v(" "),e("p",[v._v("Temporal difference learning：")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("sample = R(s,Π(s), s') + λV(s')")])]),v._v(" "),e("li",[e("code",[v._v("VΠ(s) <-- (1-a)VΠ(s) + (a)sample")])])]),v._v(" "),e("p",[v._v("以上为更新已走过节点的方法")]),v._v(" "),e("p",[v._v("每次获得新的sample，都对刚走过的状态"),e("code",[v._v("s")]),v._v("进行更新，以接近精确值")]),v._v(" "),e("p",[v._v("在这一过程中，我们从未建立世界模型，即T/R函数，只是根据样例值不断更新状态值，随着时间的推移，将得到精确值")]),v._v(" "),e("p",[v._v("优化求均值的方法，让越接近的经历比以前的经历更重要，因为我们后来计算的结果总是更加准确")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("xn = (xn + (1-a)*xn-1 + (1-a)^2*xn-2+...) / 1+(1-a)+(1-a)^2+...")])]),v._v(" "),e("p",[v._v("xn为第n个样例")])]),v._v(" "),e("li",[e("p",[v._v("这里的a为学习率，应用于迭代方程中")])])]),v._v(" "),e("p",[v._v("由于我们从未构建模型，也没有T/R函数，根本无从进行策略迭代")]),v._v(" "),e("p",[v._v("为什么不学习"),e("code",[v._v("Q-Value")]),v._v("而是"),e("code",[v._v("V-Value")]),v._v("？")]),v._v(" "),e("p",[v._v("没有理由，他不仅同样能实现更新Value，而且可以用于策略更新，属于积极的学习")]),v._v(" "),e("h5",{attrs:{id:"_5-2-2、q-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-2、q-learning"}},[v._v("#")]),v._v(" 5.2.2、Q-Learning")]),v._v(" "),e("blockquote",[e("p",[v._v("Active Reinforcement Learning")]),v._v(" "),e("p",[v._v("担心数据从何处收集，担心采取行动")])]),v._v(" "),e("p",[v._v("also")]),v._v(" "),e("ul",[e("li",[v._v("don't know the transitions T")]),v._v(" "),e("li",[v._v("don't know the reward R")]),v._v(" "),e("li",[v._v("choose the actions now（当前做的）")]),v._v(" "),e("li",[v._v("goal：learn the optimal policy/values")])]),v._v(" "),e("p",[v._v("不同于MDPs，这不是离线测试（毕竟不知道T/R，无法进行推测），而是真切地采取行动")]),v._v(" "),e("p",[v._v("iteration")]),v._v(" "),e("ul",[e("li",[v._v("从一个确定状态值开始")]),v._v(" "),e("li",[v._v("计算该状态值下一层每个状态的Q-Value和Value")]),v._v(" "),e("li",[v._v("通过下一层的Q-Value/Value更新该层的Q-Value/Value")]),v._v(" "),e("li",[v._v("迭代这一过程，更新所有Q-Value/Value")])]),v._v(" "),e("p",[v._v("Value Iteration")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("Vk+1(s) <-- maxΣT(s,a,s')[R(s,a,s') + λVk(s')]")])])]),v._v(" "),e("p",[v._v("Q-Value Iteration")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("Qk+1(s,a) <-- ΣT(s,a,s')[R(s,a,s') + λmaxQk(s',a')]")])])]),v._v(" "),e("p",[v._v("在这里使用的样例和更新策略")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("sample = R(s,a,s') + λmaxQk(s',a')")])])]),v._v(" "),e("li",[e("p",[e("code",[v._v("Q(s,a) <-- (1-a)Q(s,a) + (a)sample")])]),v._v(" "),e("p",[v._v("这个常量a称为学习率")])])]),v._v(" "),e("p",[v._v("举例：crawler bot（爬虫机器人）")]),v._v(" "),e("p",[v._v("Q-Learning is called off-policy learning")]),v._v(" "),e("p",[v._v("Caveats（警告）")]),v._v(" "),e("ul",[e("li",[v._v("have to explore enough")]),v._v(" "),e("li",[v._v("have to eventually make the learning rate small enough（收敛）")]),v._v(" "),e("li",[v._v("...but not decrease it too quickly")]),v._v(" "),e("li",[v._v("it doesn't matter how you select actions")])]),v._v(" "),e("table",[e("thead",[e("tr",[e("th",[v._v("Problem")]),v._v(" "),e("th",[v._v("Goal")]),v._v(" "),e("th",[v._v("Technique")])])]),v._v(" "),e("tbody",[e("tr",[e("td",[v._v("Known MDP")]),v._v(" "),e("td",[v._v("Compute"),e("code",[v._v("V*,Q*,Π*")]),v._v("; Evaluate a fixed policy")]),v._v(" "),e("td",[v._v("Value/Policy iteration; Policy evaluation")])]),v._v(" "),e("tr",[e("td",[v._v("Unknown MDP: Model-Based")]),v._v(" "),e("td",[v._v("Compute"),e("code",[v._v("V*,Q*,Π*")]),v._v("; Evaluate a fixed policy")]),v._v(" "),e("td",[v._v("VI/PI on approximate MDP; PE on approximate MDP")])]),v._v(" "),e("tr",[e("td",[v._v("Unknown MDP: Model-Free")]),v._v(" "),e("td",[v._v("Compute"),e("code",[v._v("V*,Q*,Π*")]),v._v("; Evaluate a fixed policy")]),v._v(" "),e("td",[v._v("Q-learning; Value learning")])])])]),v._v(" "),e("p",[v._v("均使用贝尔曼方程进行递归计算")]),v._v(" "),e("p",[v._v("Exploration（探索）vs. exploitation（开发）")]),v._v(" "),e("p",[v._v("Exploration function")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("探索未知节点，收集更多经验：random actions（ε epsilon-greedy）")]),v._v(" "),e("p",[v._v("当ε越大，随机度越高，当为0，策略确定")])]),v._v(" "),e("li",[e("p",[v._v("探索方程将根据一个节点的“经验”，如访问过多少次，来给予相应的奖励（访问越多，奖励越低）")])]),v._v(" "),e("li",[e("p",[e("code",[v._v("f(u,n) = u + k/n")]),v._v("（基数+奖励/访问次数）")])]),v._v(" "),e("li",[e("p",[v._v("这样能有效腐烂一些无用的节点（越多访问奖励越少）")])])]),v._v(" "),e("p",[v._v("Q-Update：加入探索方程")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Regular Q-Update:")]),v._v(" "),e("p",[e("code",[v._v("Q(s,a) <-- ΣT(s,a,s')[R(s,a,s') + λmaxQ(s',a')]")])])]),v._v(" "),e("li",[e("p",[v._v("Modified Q-Update:")]),v._v(" "),e("p",[e("code",[v._v("Q(s,a) <-- ΣT(s,a,s')[R(s,a,s') + λmaxf(Q(s',a'),N(s',a'))]")])])])]),v._v(" "),e("p",[v._v("Regret")]),v._v(" "),e("h5",{attrs:{id:"_5-2-3、approximate-q-learning"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-3、approximate-q-learning"}},[v._v("#")]),v._v(" 5.2.3、Approximate Q-Learning")]),v._v(" "),e("p",[v._v("在实际问题中，状态数、动作会很多很多，很难在Q-Table中去储存每一个Q-Value，这个时候只能做估计")]),v._v(" "),e("p",[v._v("w为权重，f为特征值（features）")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("V(s) = w1*f1(s)+w2*f2(s)+...+wn*fn(s)")])]),v._v(" "),e("li",[e("code",[v._v("Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)")])])]),v._v(" "),e("p",[v._v("你的Q值将是很多经验的加权和，如f1为跳楼的特征值，f2为纵火的特征值，Q将这些情况的经验汇总以某些权重组合")]),v._v(" "),e("p",[v._v("当特征值"),e("code",[v._v(">1")]),v._v("说明他鼓励这种差异，反之对差异持消极态度")]),v._v(" "),e("p",[v._v("a仍是学习率")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("Q(s,a) <-- Q(s,a) + a[diff]")])]),v._v(" "),e("p",[v._v("准确的Q值")])]),v._v(" "),e("li",[e("p",[e("code",[v._v("wi <-- wi + a[diff]f(s,a)")])]),v._v(" "),e("p",[v._v("近似的Q值")])])]),v._v(" "),e("p",[v._v("当权重降低，其对应的多项式变低，Q得到调整，那么更新权重成为现在的问题")]),v._v(" "),e("p",[v._v("这么做的目的无非是想用相对少的数据得到一个相对好的Q函数")]),v._v(" "),e("p",[e("strong",[v._v("Optimization")])]),v._v(" "),e("p",[v._v("最小二乘法处理特征值"),e("code",[v._v("features")])]),v._v(" "),e("ul",[e("li",[e("code",[v._v("Q(s,a) = w1*f1(s,a)+w2*f2(s,a)+...+wn*fn(s,a)")])]),v._v(" "),e("li",[e("code",[v._v("Q(s,a)=w0 + w1f1(s,a)")])])]),v._v(" "),e("p",[v._v("Minimizing Error")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("error(w) = (1/2)*(y-Σwk*fk(x))½")])])]),v._v(" "),e("p",[v._v("对该函数对w求导得")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("-(y-Σwk*fk(x))fm(x)")])])]),v._v(" "),e("p",[v._v("Why limiting capacity can help?")]),v._v(" "),e("p",[v._v("功能越多并不一定越好，这意味着更高阶的多项式，在函数曲线上更加符合")]),v._v(" "),e("p",[v._v("这有可能造成过度拟合（overfitting），即为了满足一些离谱的数据，做出疯狂的拟合")]),v._v(" "),e("h5",{attrs:{id:"_5-2-4、policy-search"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-4、policy-search"}},[v._v("#")]),v._v(" 5.2.4、Policy Search")]),v._v(" "),e("p",[v._v("尝试不同的策略，看哪一个更好")]),v._v(" "),e("p",[v._v("Q-Learning：Q值接近，无法确定这是最好的行动")]),v._v(" "),e("p",[v._v("让我们关注行动")]),v._v(" "),e("p",[v._v("我们有一些Qvalue，向上向下调整特征值权重，看看有什么变化，好则接收，坏则丢弃，然后继续调整，就像CSP的本地搜索")]),v._v(" "),e("blockquote",[e("p",[v._v("直升飞机倒挂着飞会省四倍阻力")]),v._v(" "),e("p",[v._v("ai vs. ai and train each other")])]),v._v(" "),e("h2",{attrs:{id:"二、不确定知识和推理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#二、不确定知识和推理"}},[v._v("#")]),v._v(" 二、不确定知识和推理")]),v._v(" "),e("blockquote",[e("p",[v._v("Probabilistic Reasoning")])]),v._v(" "),e("h3",{attrs:{id:"_1、probability"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1、probability"}},[v._v("#")]),v._v(" 1、Probability")]),v._v(" "),e("blockquote",[e("p",[v._v("概率论：不确定性下建模和计算以及人工智能的基础")])]),v._v(" "),e("p",[v._v("example：捉鬼敢死队")]),v._v(" "),e("ul",[e("li",[v._v("在一个二维的方阵中，隐藏了一个或多个鬼")]),v._v(" "),e("li",[v._v("探针标记坐标：颜色越深鬼越近，但有概率出错，每个点只能标记一次，并且每次标记负分")]),v._v(" "),e("li",[v._v("开枪：结束游戏，若打到鬼胜利，若为打到则失败")])]),v._v(" "),e("p",[v._v("Models：在这里指巨大的联合分布，一个巨大的概率查找表")]),v._v(" "),e("p",[e("strong",[v._v("Random Variables")])]),v._v(" "),e("p",[v._v("随机变量")]),v._v(" "),e("p",[v._v("简单分类")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("R = is it raining（treu/false）")])]),v._v(" "),e("li",[e("p",[v._v("T = is it hot or cold（hot/cold）")])]),v._v(" "),e("li",[e("p",[v._v("D = how long will it take to drive to work（[0, x)）")]),v._v(" "),e("p",[v._v("连续性随机变量")])]),v._v(" "),e("li",[e("p",[v._v("L = where is ghost（{0，0}，{0，1}...）")])])]),v._v(" "),e("p",[v._v("有点像CSP问题中"),e("code",[v._v("state & domain")]),v._v("（状态和值域）")]),v._v(" "),e("p",[v._v("通常随机变量都是离散型变量")]),v._v(" "),e("p",[v._v("在真实模型中随机变量概率为0是毁灭性的，概率总和为1")]),v._v(" "),e("p",[v._v("简记")]),v._v(" "),e("ul",[e("li",[v._v("P(hot) = P(T=hot)")])]),v._v(" "),e("p",[e("strong",[v._v("Joint distributions")])]),v._v(" "),e("blockquote",[e("p",[v._v("联合分布：概率模型的核心")]),v._v(" "),e("p",[v._v("给出推荐概率的关键")])]),v._v(" "),e("p",[v._v("联合分布概率，即约束一系列随机变量值的概率，就像真值表")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("P(X1=x1,X2=x2,...,Xn=xn)")])]),v._v(" "),e("li",[e("code",[v._v("P(x1,x2,...xn)")])])]),v._v(" "),e("p",[v._v("n variables with n domain distrbution size：d^n")]),v._v(" "),e("p",[v._v("永远不能写出整个概率空间")]),v._v(" "),e("p",[e("strong",[v._v("Probabilistic Models")])]),v._v(" "),e("p",[v._v("概率模型，一组变量和其概率，像是一个巨大的CSP问题，变量对应状态，概率对应值，各变量间存在约束")]),v._v(" "),e("p",[e("strong",[v._v("Event")])]),v._v(" "),e("p",[v._v("事件是一组随机变量的联合结果，通过联合概率表我们可以计算任一事件的概率")]),v._v(" "),e("p",[e("strong",[v._v("Marginal Distributions")])]),v._v(" "),e("p",[v._v("边缘分布：联合分布的一个子表格")]),v._v(" "),e("p",[v._v("如有随机变量x、y及其联合分布表a，我们单独提出x的概率分布表b，那么b就是a的一个边缘分布")]),v._v(" "),e("p",[e("strong",[v._v("Conditional Probabilities")])]),v._v(" "),e("p",[v._v("条件概率，a、b均为某一事件")]),v._v(" "),e("ul",[e("li",[v._v("P(a|b) = P(a, b) / P(b)")])]),v._v(" "),e("p",[v._v("即在b发生的前提下，a发生的概率，可以由联合概率和边缘概率计算得出")]),v._v(" "),e("p",[e("strong",[v._v("Conditional Distributions")])]),v._v(" "),e("p",[v._v("条件分布，这里W、T均为变量，条件分布将是一组概率集，即在T为hot的前提下，变量W的概率分布关系")]),v._v(" "),e("ul",[e("li",[v._v("P(W|T=hot)")])]),v._v(" "),e("p",[v._v("他依据事件发生的事实（具体值），而不是概率分布表，这有区别于边缘分布")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("P(W=s|T=c) = P(W=s, T=c) / ΣP(W=wi, T=c)")])]),v._v(" "),e("p",[v._v("即某一确定条件概率为其条件和自身都发生的概率除以所有可能发生的概率总和")])])]),v._v(" "),e("p",[v._v("所有这些事件的概率集合便是W|T的条件分布表")]),v._v(" "),e("p",[v._v("计算条件分布的一般方式：")]),v._v(" "),e("blockquote",[e("p",[v._v("the algorithms for base nets")])]),v._v(" "),e("ul",[e("li",[v._v("在概率分布表中选出条件满足的所有行")]),v._v(" "),e("li",[v._v("等比例调整他们的概率使其概率和为1")])]),v._v(" "),e("p",[e("strong",[v._v("Probabilistic Inference")])]),v._v(" "),e("blockquote",[e("p",[v._v("概率推理")])]),v._v(" "),e("p",[v._v("从已知的概率基于证据推理未知的概率，得到期望的概率")]),v._v(" "),e("p",[e("strong",[v._v("Inference by Enumeration")])]),v._v(" "),e("p",[v._v("枚举推理")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Evdence variables：E1...Ek=e1...ek")]),v._v(" "),e("p",[v._v("已知事实")])]),v._v(" "),e("li",[e("p",[v._v("Query* variable：Q")]),v._v(" "),e("p",[v._v("查询变量，即要推理的问题，想要计算的结果")])]),v._v(" "),e("li",[e("p",[v._v("Hidden variables：H1...Hr")]),v._v(" "),e("p",[v._v("尚未证实的事实，也属于未知变量")])])]),v._v(" "),e("p",[v._v("推理过程")]),v._v(" "),e("p",[v._v("step 1：select the entries consistent with the evidence")]),v._v(" "),e("ul",[e("li",[v._v("查询出一个与Q有关的联合概率分布表")])]),v._v(" "),e("p",[v._v("step 2：sum out H to get joint（联合） of Query and evidence")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("Query")]),v._v(" "),e("p",[v._v("根据Q合并相同特征的行。如Q为推断天气情况，那么温度和季节将是隐藏变量，你将在使隐藏变量塌陷，将所有"),e("code",[v._v("W=sun")]),v._v("和"),e("code",[v._v("W=rain")]),v._v("提出并合并成两行（subtable）")])]),v._v(" "),e("li",[e("p",[v._v("Evidence")]),v._v(" "),e("p",[v._v("根据已知证据（条件），筛选有用的概率分布，如已知是冬天，那么将去掉所有"),e("code",[v._v("S!=冬天")]),v._v("的行")]),v._v(" "),e("p",[v._v("条件越多，筛选结果越少")])])]),v._v(" "),e("p",[v._v("step 3：normalize")]),v._v(" "),e("ul",[e("li",[v._v("即将概率总和等比率扩大至1，转化为一个规范的概率表")])]),v._v(" "),e("p",[v._v("隐藏变量均具有转化为证据的潜力")]),v._v(" "),e("p",[v._v("Obvious problems")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("worst case time complexity O(d^n)")]),v._v(" "),e("p",[v._v("查询整张联合概率分布表")])]),v._v(" "),e("li",[e("p",[v._v("space complexity O(d^n) to store the joint distribution")]),v._v(" "),e("p",[v._v("需要O(d^n)的空间储存联合概率表")])])]),v._v(" "),e("p",[v._v("How to produce joint distribution?")]),v._v(" "),e("p",[e("strong",[v._v("The Product Rule")])]),v._v(" "),e("ul",[e("li",[e("code",[v._v("P(y) * P(x|y) = P(x,y)")])])]),v._v(" "),e("p",[v._v("根据局部条件分布推断联合概率分布")]),v._v(" "),e("p",[e("strong",[v._v("The Chain Rule")])]),v._v(" "),e("p",[v._v("连锁规则，对于"),e("code",[v._v("P(y) * P(x|y) = P(x,y)")]),v._v("的推广")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("P(x1,x2,x3) = P(x1) * P(x2|x1) * P(x3|x1x2)")])]),v._v(" "),e("li",[e("code",[v._v("P(x1,x2,...,xn) = ∏P(xi|x1...xi-1)")])])]),v._v(" "),e("p",[v._v("先获得"),e("code",[v._v("P(x1)")]),v._v("，通过条件"),e("code",[v._v("P(x1),P(x2|x1)")]),v._v("计算"),e("code",[v._v("P(x2,x1)")]),v._v("，通过"),e("code",[v._v("P(x2,x1),P(x3|x1,x2)")]),v._v("获得"),e("code",[v._v("P(x3,x1,x2)")]),v._v("，以此类推，最终可以获得"),e("code",[v._v("P(x1,x2,...,xn)")])]),v._v(" "),e("p",[e("strong",[v._v("Bayes Rule")])]),v._v(" "),e("p",[v._v("Product rule："),e("code",[v._v("P(x,y) = P(x|y)P(y) = P(y|x)P(x)")])]),v._v(" "),e("p",[v._v("移项可得："),e("code",[v._v("P(x|y) = (P(y|x)/P(y)) * P(x)")])]),v._v(" "),e("p",[v._v("why it's helpful?")]),v._v(" "),e("p",[v._v("通过转换条件（在反向条件中构建条件），通过更简单的条件进行建模")]),v._v(" "),e("p",[e("strong",[v._v("Inference with Bayes' Rule")])]),v._v(" "),e("p",[v._v("例子：诊断病例"),e("code",[v._v("P(致病因素|症状)")])]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("P(cause|effect) = (P(effect|cause)P(cause)) / P(effect)")])])]),v._v(" "),e("li",[e("p",[v._v("cause：脑膜炎")])]),v._v(" "),e("li",[e("p",[v._v("effect：颈部僵硬")])])]),v._v(" "),e("p",[v._v("已知：颈部僵硬在普通人群中概率，颈部僵硬在脑膜炎患者之间的概率")]),v._v(" "),e("p",[v._v("当你不知道得没得脑膜炎时，因为没有症状，但突然间脖子僵硬了，你就需要重新考虑你得脑膜炎的概率")]),v._v(" "),e("p",[v._v("从果溯因的思想，从更多的结果不断更新因的概率使之准确")]),v._v(" "),e("p",[e("strong",[v._v("Probabilistic Models")])]),v._v(" "),e("blockquote",[e("p",[v._v("概率建模")])]),v._v(" "),e("ul",[e("li",[v._v("describe how (a part of) the world works")])]),v._v(" "),e("p",[v._v("建模总是一个简化过程")]),v._v(" "),e("ul",[e("li",[v._v("从哪获取变量")]),v._v(" "),e("li",[v._v("怎么计算概率")]),v._v(" "),e("li",[v._v("如何学习")])]),v._v(" "),e("blockquote",[e("p",[v._v("All models are wrong, but some are useful")]),v._v(" "),e("p",[v._v("——Geoge E. P. Box")])]),v._v(" "),e("p",[v._v("我们的任务便是提出有用的模型")]),v._v(" "),e("p",[e("strong",[v._v("Utility")])]),v._v(" "),e("blockquote",[e("p",[v._v("模型的效用")])]),v._v(" "),e("ul",[e("li",[v._v("推理未知变量")]),v._v(" "),e("li",[v._v("预测，因果推理")]),v._v(" "),e("li",[v._v("通过已知变量做出理性决定")])]),v._v(" "),e("p",[e("strong",[v._v("independence")])]),v._v(" "),e("blockquote",[e("p",[v._v("随机变量的独立性")])]),v._v(" "),e("ul",[e("li",[e("code",[v._v("P(x,y) = P(x)P(y)")])]),v._v(" "),e("li",[e("code",[v._v("P(x|y) = P(y)")])])]),v._v(" "),e("p",[v._v("条件概率等于自身概率，就是说y发不发生对于x是否发生不产生影响，x、y相互独立，是两个一维表，而不是一个联合概率二维表，如抛硬币字面和花面互不影响，二者独立")]),v._v(" "),e("p",[v._v("很多时候很多事都不是完全独立的")]),v._v(" "),e("p",[v._v("如何检验是否独立？")]),v._v(" "),e("p",[v._v("边缘概率相乘是否等于联合概率，若相等则独立，若不等则相关")]),v._v(" "),e("p",[v._v("像抛硬币这样的独立情况是很罕见的，我们可以只通过一组概率推断一个很大的概率分布表（因为每次都不变）")]),v._v(" "),e("ul",[e("li",[v._v("变量往往有互动，因为你在建模的时候，为什么会选择两个完全无关的变量参与呢")]),v._v(" "),e("li",[v._v("更多时候是有条件独立（Conditional Independence）")])]),v._v(" "),e("p",[e("strong",[v._v("Conditional Independence")])]),v._v(" "),e("blockquote",[e("p",[v._v("条件独立")])]),v._v(" "),e("p",[v._v("呃就是说，a和b的相关需要条件c，比如a是发现蛀牙，b是牙疼，c是你有牙")]),v._v(" "),e("p",[v._v("那么，"),e("code",[v._v("P(a|b) != P(a)")]),v._v("当且仅当"),e("code",[v._v("c")]),v._v("成立，否则"),e("code",[v._v("P(a|b) = P(a)")]),v._v("，即a、b相互独立")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("牙疼和发现蛀牙是独立的当没有牙")]),v._v(" "),e("p",[v._v("toothcache ⊥ catch | no tooth")])])]),v._v(" "),e("p",[v._v("条件独立：")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("P(x,y|z) = P(x|z)P(y|z)")])])]),v._v(" "),e("li",[e("p",[e("code",[v._v("P(x|z,y) = P(x|z)")])]),v._v(" "),e("p",[v._v("在z发生时，x、y独立，y是否发生不影响x发生概率")])])]),v._v(" "),e("p",[v._v("think about these domain")]),v._v(" "),e("ul",[e("li",[v._v("traffic")]),v._v(" "),e("li",[v._v("umbrella")]),v._v(" "),e("li",[v._v("raining")])]),v._v(" "),e("p",[v._v("在下雨时交通和雨伞相关联，不下雨时相互独立")]),v._v(" "),e("ul",[e("li",[v._v("fire")]),v._v(" "),e("li",[v._v("smoke")]),v._v(" "),e("li",[v._v("alarm")])]),v._v(" "),e("p",[v._v("当没有烟雾，火灾和警报将互不关联（烟雾报警器）")]),v._v(" "),e("p",[v._v("对于一个Chain Rule")]),v._v(" "),e("ul",[e("li",[e("code",[v._v("P(x,y,z) = P(z)P(y|z)P(x|z,y)")])])]),v._v(" "),e("p",[v._v("若x、y关于z条件独立，那么这个规则可以简化为")]),v._v(" "),e("ul",[e("li",[e("p",[e("code",[v._v("P(x,y,z) = P(z)P(y|z)P(x|z)")])]),v._v(" "),e("p",[v._v("即将"),e("code",[v._v("P(x|z,y)")]),v._v("简化为"),e("code",[v._v("P(x|y)")])])])]),v._v(" "),e("p",[v._v("这样我们能通过更少的变量主键确定一张联合概率表")]),v._v(" "),e("h3",{attrs:{id:"_2、bayes-nets"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2、bayes-nets"}},[v._v("#")]),v._v(" 2、Bayes' Nets")]),v._v(" "),e("blockquote",[e("p",[v._v("贝叶斯网络")]),v._v(" "),e("p",[v._v("一种用于构建概率模型的技术，管理和推理超过规模的不确定性")])]),v._v(" "),e("h4",{attrs:{id:"_2-1、representation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-1、representation"}},[v._v("#")]),v._v(" 2.1、Representation")]),v._v(" "),e("blockquote",[e("p",[v._v("表示法")])]),v._v(" "),e("p",[e("strong",[v._v("Big Picture")])]),v._v(" "),e("p",[v._v("概率模型等于一整个概率分布表？")]),v._v(" "),e("ul",[e("li",[v._v("数学上这很好")]),v._v(" "),e("li",[v._v("大小是指数型的，真的很大")]),v._v(" "),e("li",[v._v("很难去进行学习、评估")])]),v._v(" "),e("p",[e("strong",[v._v("Example")])]),v._v(" "),e("p",[v._v("Insurance：汽车保险")]),v._v(" "),e("ul",[e("li",[v._v("a lot of variables：age、car、skill...")]),v._v(" "),e("li",[v._v("accident：want to prediction")])]),v._v(" "),e("p",[v._v("所有这些变量根据联系构建了一张大的关系网")]),v._v(" "),e("p",[e("strong",[v._v("Graphical Model Notation")])]),v._v(" "),e("blockquote",[e("p",[v._v("图形模式表示法")])]),v._v(" "),e("p",[v._v("just like CSPs")]),v._v(" "),e("ul",[e("li",[v._v("nodes：变量，variables")]),v._v(" "),e("li",[v._v("domains：值域，变量的值")]),v._v(" "),e("li",[v._v("arcs：用弧表示条件独立性")])]),v._v(" "),e("p",[e("strong",[v._v("Example")])]),v._v(" "),e("p",[v._v("1、Coin Flips")]),v._v(" "),e("p",[v._v("硬币翻转问题，绝对的独立性，对应一堆没有弧线的节点")]),v._v(" "),e("p",[v._v("2、Traffic")]),v._v(" "),e("p",[v._v("交通问题，变量为天气和交通")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("model 1：independence")])]),v._v(" "),e("li",[e("p",[v._v("model 2：rain causes traffic")]),v._v(" "),e("p",[v._v("箭头从下雨指向交通")])])]),v._v(" "),e("p",[v._v("model 2 is better")]),v._v(" "),e("p",[v._v("3、Traffic Ⅱ")]),v._v(" "),e("p",[v._v("交通问题Ⅱ，变量有交通T、下雨R、低气压L、屋顶滴雨D、一个球赛B、蛀牙C")]),v._v(" "),e("ul",[e("li",[v._v("低气压影响下雨")]),v._v(" "),e("li",[v._v("下雨影响交通、屋顶滴雨、球赛")]),v._v(" "),e("li",[v._v("球赛影响交通")]),v._v(" "),e("li",[v._v("蛀牙独立于其他变量")])]),v._v(" "),e("p",[v._v("4、Alarm Network")]),v._v(" "),e("p",[v._v("报警装置，变量有入室盗窃B、警报响起A、玛丽打电话来M、约翰打电话来J、地震E")]),v._v(" "),e("ul",[e("li",[v._v("入室盗窃导致警报响起")]),v._v(" "),e("li",[v._v("地震导致警报响起")]),v._v(" "),e("li",[v._v("警报响起导致玛丽和约翰打电话来")])]),v._v(" "),e("p",[e("strong",[v._v("Bayes' Net Semantics")])]),v._v(" "),e("blockquote",[e("p",[v._v("贝叶斯网络定义：一种拓扑结构")])]),v._v(" "),e("p",[v._v("贝叶斯网络是一组节点，每个变量x对应一个节点")]),v._v(" "),e("p",[v._v("在这些节点中有一个有向无环图，对应变量间的联系（因果关系），注意图中没有定向循环")]),v._v(" "),e("p",[v._v("一个节点可能有多个因，果继承了因的某些功能并且呈指数增长，我理解为条件概率在传递过程中不断变化")]),v._v(" "),e("p",[v._v("每个节点的概率都是相对其父母节点的条件概率，整合所有的条件概率可以获得一张联合概率表，通过链规则或其他")]),v._v(" "),e("p",[v._v("CPT：条件概率表（conditional probability table）")]),v._v(" "),e("p",[v._v("不一定是因果关系，异或等都有可能")]),v._v(" "),e("p",[e("strong",[v._v("Probabilities in BNs")])]),v._v(" "),e("p",[v._v("基于Chain Rule，链规则的扩展")]),v._v(" "),e("p",[e("code",[v._v("P(x1,x2,...,xn) = ∏ P(xi|parents(Xi))")])]),v._v(" "),e("p",[v._v("贝叶斯网络提供保证，通过乘积将得到节点概率")]),v._v(" "),e("p",[v._v("可以通过一个拓扑图将上述例子画出，并提供相应的计算方式")]),v._v(" "),e("p",[v._v("在贝叶斯网中，因果关系被弱化，由数字和字母表示，程序并不知道这些数字和字母背后的因果，只是得到一张巨大的联合概率分布表")]),v._v(" "),e("h4",{attrs:{id:"_2-2、conditional-independence"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2、conditional-independence"}},[v._v("#")]),v._v(" 2.2、Conditional Independence")]),v._v(" "),e("blockquote",[e("p",[v._v("条件独立")])]),v._v(" "),e("p",[e("strong",[v._v("Size of a Bayes' Net")])]),v._v(" "),e("p",[v._v("N Boolean variables："),e("code",[v._v("2^N")])]),v._v(" "),e("p",[v._v("N-node net and nodes have up to k parents："),e("code",[v._v("O(N*2^(k+1))")])]),v._v(" "),e("p",[v._v("若单纯的计算联合概率分布，指数增长将令算法陷入困境")]),v._v(" "),e("p",[v._v("贝叶斯网络将有效缓解")]),v._v(" "),e("ul",[e("li",[v._v("条件概率表远小于完整联合概率表")])]),v._v(" "),e("p",[e("strong",[v._v("Assumptions")])]),v._v(" "),e("p",[v._v("我们总通过条件概率的乘积来表示当前变量概率，不同于链规则调整所有的条件，贝叶斯网络只根据父级概率进行调整，只关心父母节点")]),v._v(" "),e("p",[v._v("举个例子：X ——> Y ——> Z ——> W")]),v._v(" "),e("p",[v._v("Chain Rule："),e("code",[v._v("P(x,y,z,w) = P(x)P(y|x)P(z|x,y)P(w|x,y,z)")])]),v._v(" "),e("p",[v._v("NBs："),e("code",[v._v("P(x,y,z,w) = P(x)P(y|x)P(z|y)P(w|z)")])]),v._v(" "),e("ul",[e("li",[v._v("基于条件独立的简化")]),v._v(" "),e("li",[v._v("条件独立是可以传递的，如这里w和z条件独立与y，我们也可以说w和x条件独立与y或z")])]),v._v(" "),e("p",[v._v("另一个例子：X ——> Y ——> Z")]),v._v(" "),e("p",[v._v("X和Z保证独立吗？")]),v._v(" "),e("p",[v._v("他们不保证独立，X可以影响Z当Y发生，如低气压引发下雨，下雨引发堵车，那么我们有结论则低气压可以引发堵车当低气压引发了下雨")]),v._v(" "),e("p",[v._v("当X发生时，Y发生概率为0时，X和Z保持独立，就像X为抛硬币字面，Y为花面，那么我们可以说这个贝叶斯网络有点智障")]),v._v(" "),e("h5",{attrs:{id:"_2-2-1、d-separation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-1、d-separation"}},[v._v("#")]),v._v(" 2.2.1、D-Separation")]),v._v(" "),e("blockquote",[e("p",[v._v("D分离")])]),v._v(" "),e("p",[e("strong",[v._v("Causal Chains")])]),v._v(" "),e("p",[v._v("因果链")]),v._v(" "),e("p",[v._v("例子：X（low pressure） ——> Y （rain）——> Z（traffic）")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("无法保证X/Z独立")])]),v._v(" "),e("li",[e("p",[v._v("能够保证X/Z关于条件Y独立，得到简化")]),v._v(" "),e("p",[e("code",[v._v("P(z|x,y) = P(x,y,z)/P(x,y) = P(x)P(y|x)P(z|y)/P(x)P(y|x) = P(z|y)")])]),v._v(" "),e("p",[v._v("从公式中可以看到当前节点概率只与其父母节点有关")])])]),v._v(" "),e("p",[e("strong",[v._v("Common Cause")])]),v._v(" "),e("p",[v._v("公共条件，即一个诱因对应多个结果，两或多个个效果由同一个条件导致")]),v._v(" "),e("p",[v._v("条件概率选择")]),v._v(" "),e("p",[v._v("例子：X ——> Y ——> Z/W")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("通常X/Z不会独立")])]),v._v(" "),e("li",[e("p",[v._v("X和Z/W关于Y条件独立，当Y发生，X肯定发生，不对Z/W造成任何影响")]),v._v(" "),e("p",[e("code",[v._v("P(z|x,y) = P(z|y)")])])])]),v._v(" "),e("p",[e("strong",[v._v("Common Effect")])]),v._v(" "),e("p",[v._v("相同效果，即多个诱因可能导致相同的结果。一个结果可以由多个因素导致")]),v._v(" "),e("p",[v._v("例子：X/Y ——> Z")]),v._v(" "),e("ul",[e("li",[e("p",[v._v("在这个例子中X/Y是否独立")]),v._v(" "),e("p",[v._v("yes")])]),v._v(" "),e("li",[e("p",[v._v("X/Y是否关于Z条件独立")]),v._v(" "),e("p",[v._v("no，当Z发生，X/Y都有可能发生，当拥有证据X发生了，那么Y的发生概率将会降低")])])]),v._v(" "),e("p",[e("strong",[v._v("The General Case")])]),v._v(" "),e("p",[v._v("一般情况，以三元组为例")]),v._v(" "),e("p",[v._v("Reachability：可到达性")]),v._v(" "),e("p",[v._v("通常在贝叶斯网络中将隐藏一些证据节点，观察贝叶斯图中其他节点的结果进行推断")]),v._v(" "),e("p",[v._v("Active/Inactive Paths")]),v._v(" "),e("ul",[e("li",[v._v("链结构：中间无证据，活跃")]),v._v(" "),e("li",[e("code",[v._v("V")]),v._v("结构：中间有证据，活跃")]),v._v(" "),e("li",[e("code",[v._v("^")]),v._v("结构：中间无证据，活跃")])]),v._v(" "),e("blockquote",[e("p",[v._v("以上为D-Separation的轮廓")])]),v._v(" "),e("p",[v._v("Query："),e("code",[v._v("Xi⊥Xj | {Xk1,...,Xkn}")])]),v._v(" "),e("p",[v._v("Check：检查Xi到Xj间的所有路径，若有任一一条路径是活跃的，那么Xi和Xj不相互独立；否则二者独立")]),v._v(" "),e("p",[v._v("Example：")]),v._v(" "),e("p",[v._v("R/B ——> T ——> T'")]),v._v(" "),e("ul",[e("li",[v._v("R⊥B：yes（独立）")]),v._v(" "),e("li",[v._v("R⊥B|T：no（关联）")]),v._v(" "),e("li",[v._v("R⊥B|T'：no（关联）")])]),v._v(" "),e("p",[v._v("L ——> R ——> D/T，B ——> T ——> T'")]),v._v(" "),e("ul",[e("li",[v._v("L⊥T'|T：yes（R-T-T'为非活跃）")]),v._v(" "),e("li",[v._v("L⊥B：yes")]),v._v(" "),e("li",[v._v("L⊥B|T：no（条件相关）")]),v._v(" "),e("li",[v._v("L⊥B|T'：no")]),v._v(" "),e("li",[v._v("L⊥B|T,R：yes")])]),v._v(" "),e("p",[v._v("R ——> T/D，T/D ——> S")]),v._v(" "),e("ul",[e("li",[v._v("T⊥D：no")])]),v._v(" "),e("h4",{attrs:{id:"_2-3、inference"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-3、inference"}},[v._v("#")]),v._v(" 2.3、Inference")]),v._v(" "),e("h4",{attrs:{id:"_2-4、sampling"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-4、sampling"}},[v._v("#")]),v._v(" 2.4、Sampling")]),v._v(" "),e("h2",{attrs:{id:"三、机器学习"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#三、机器学习"}},[v._v("#")]),v._v(" 三、机器学习")])])}),[],!1,null,null,null);_.default=a.exports}}]);